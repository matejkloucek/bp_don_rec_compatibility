#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
%% Font setup: please leave the LyX font settings all set to 'default'
%% if you want to use any of these packages:

%% Use Times New Roman font for text and Belleek font for math
%% Please make sure that the 'esint' package is turned off in the
%% 'Math options' page.
\usepackage[varg]{txfonts}

%% Use Utopia text with Fourier-GUTenberg math
%\usepackage{fourier}

%% Bitstream Charter text with Math Design math
%\usepackage[charter]{mathdesign}

%%---------------------------------------------------------------------

%% Make the multiline figure/table captions indent so that the second
%% line "hangs" right below the first one.
%\usepackage[format=hang]{caption}

%% Indent even the first paragraph in each section
\usepackage{indentfirst}

%%---------------------------------------------------------------------

%% Disable page numbers in the TOC. LOF, LOT (TOC automatically
%% adds \thispagestyle{chapter} if not overriden
%\addtocontents{toc}{\protect\thispagestyle{empty}}
%\addtocontents{lof}{\protect\thispagestyle{empty}}
%\addtocontents{lot}{\protect\thispagestyle{empty}}

%% Shifts the top line of the TOC (not the title) 1cm upwards 
%% so that the whole TOC fits on 1 page. Additional page size
%% adjustment is performed at the point where the TOC
%% is inserted.
%\addtocontents{toc}{\protect\vspace{-1cm}}

%%---------------------------------------------------------------------

% completely avoid orphans (first lines of a new paragraph on the bottom of a page)
\clubpenalty=9500

% completely avoid widows (last lines of paragraph on a new page)
\widowpenalty=9500

% disable hyphenation of acronyms
\hyphenation{CDFA HARDI HiPPIES IKEM InterTrack MEGIDDO MIMD MPFA DICOM ASCLEPIOS MedInria}

%%---------------------------------------------------------------------

%% Print out all vectors in bold type instead of printing an arrow above them
\renewcommand{\vec}[1]{\boldsymbol{#1}}

% Replace standard \cite by the parenthetical variant \citep
%\renewcommand{\cite}{\citep}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
\end_modules
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref true
\pdf_title "Compatibility"
\pdf_author "Matěj Klouček"
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 4cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 0.8cm
\headsep 1cm
\footskip 0.5cm
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style swedish
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle headings
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
def
\backslash
documentdate{August 2, 2023}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

%%
\backslash
def
\backslash
documentdate{
\backslash
today}
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{empty}
\end_layout

\begin_layout Plain Layout

{
\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
\align block
\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "3cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Images/TITLE/cvut.pdf
	display false
	width 3cm
	height 3cm
	keepAspectRatio

\end_inset


\end_layout

\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "60line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\align center

\shape smallcaps
\size large
Czech Technical University in Prague
\shape default

\begin_inset Newline newline
\end_inset

Faculty of Nuclear Sciences and Physical Engineering
\end_layout

\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "3cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Images/TITLE/fjfi.pdf
	display false
	width 3cm
	height 3cm
	keepAspectRatio

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 3cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
Estimating Kidney Transplantation Donor-Recipient Compatibility Using Machine
 Learning
\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
\lang czech
Odhad kompatibility dárce a příjemce pro transplantaci ledvin pomocí strojového
 učení
\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\size large
Bachelor's Degree Project
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
ends the centered part (the required new paragraph before "}" is inserted
 by \SpecialChar LyX
 as "}" is on a separate line.)
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Author: 
\series bold
Matěj Klouček
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Supervisor: 
\series bold
Ing.
 Tomáš Kouřim
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Consultant: 
\series bold
Ing.
 Pavel Strachota , Ph.D.
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Academic
\begin_inset space ~
\end_inset

year: 2022/2023
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Final dummy paragraph.
 Its function is to bear the page break flag
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset External
	template PDFPages
	filename zadani.pdf
	extra LaTeX "pages=-"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\size larger
\emph on
Acknowledgment:
\end_layout

\begin_layout Standard
\noindent
I would like to thank ............................................
 for (his/her expert guidance) and expres´s my gratitude to ..........................................
 for (his/her language assistance).
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
vfill
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent

\size larger
\emph on
Author's declaration:
\end_layout

\begin_layout Standard
\noindent
I declare that this Bachelor's Degree Project is entirely my own work and
 I have listed all the used sources in the bibliography.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent
Prague, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
documentdate
\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset

Matěj Klouček
\end_layout

\begin_layout Standard
\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\emph on
\lang czech
Název práce:
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\series bold
\lang czech
Odhad kompatibility dárce a příjemce pro transplantaci ledvin pomocí strojového
 učení
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Autor:
\emph default
 Matěj Klouček
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Obor:
\emph default
 Matematické inženýrství
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Zaměření:
\emph default
 Matematická informatika
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Druh práce:
\emph default
 Bakalářská práce
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Vedoucí práce:
\emph default
 Ing.
 Tomáš Kouřim, Mild Blue, s.r.o.
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Konzultant:
\emph default
 Ing.
 Pavel Strachota , Ph.D., Katedra matematiky FJFI ČVUT
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Abstrakt:
\emph default
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Klíčová slova:
\emph default
 analýza přežívání, random survival forests, strojové učení, transplantace
 ledvin
\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\emph on
Title:
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\series bold
Estimating kidney transplantation donor-recipient compatibility using machine
 learning
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Author:
\emph default
 Matěj Klouček
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Abstract:
\emph default
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Key words:
\emph default
 Machine learning, Random survival forests, Renal transplantation, Survival
 analysis
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{plain}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter*
Introduction
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{Introduction}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Kidney transplantation is the single best treatment for patients with end-stage
 kidney disease as it has demonstrated the best quality of life improvement
 and the best survival rate amongst other possible treatments 
\begin_inset CommandInset citation
LatexCommand cite
key "Chronic kidney allograft loss,Systematic review"
literal "false"

\end_inset

.
 However, demand for allografts greatly exceeds supply 
\begin_inset CommandInset citation
LatexCommand cite
key "Systematic review"
literal "false"

\end_inset

 and finding a compatible donor-recipient pair is often a highly time-consuming
 task, which results in patients spending a prolonged time on the waiting
 list for potential transplantation.
 While on the waiting list, patients have to undergo dialysis multiple times
 a week for several hours, which supplements the functions of their failing
 kidneys.
 This is problematic not only because it greatly reduces the quality of
 the patient's life and puts an additional burden on the the healthcare
 system, but because time spent on dialysis also reduces the expected survival
 time after a potential tranplantation 
\begin_inset CommandInset citation
LatexCommand cite
key "Dialysis"
literal "false"

\end_inset

.
 It is therefore crucial to minimize the time patients spend on the waiting
 list.
\end_layout

\begin_layout Standard
Even more important than the swiftness of the donor-recipient match making,
 is the quality of the given match.
 Currently, only a handful of factors are taken into consideration when
 looking for suitable donor-recipient pair, such as whether they are immunologic
ally compatible, their medical histories and their blood type.
 In recents years, imporovements in these techniques have succesfuly minimised
 the risk of an acute transplantation rejection, which happens the recipient's
 immune system percieves the graft as a foreign, withnin the first year
 of the tranplantation below 
\begin_inset Formula $15\%$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Chronic kidney allograft loss"
literal "false"

\end_inset

.
 However, there may be other factors that influence the graft's long term
 performance that are currently not accounted for.
 Therefore, in order to reduce the ﬂow of people returning to the waiting
 list because of failed grafts, it is necessary to uncover these patterns
 and take them into account when computing compatibility between potential
 donors and recipients.
 This is where machine learning can play a crucial role, by analyzing large
 amounts of data and identifying patterns that can help find compatible
 pairs or conversely detect incompatible pairings that would otherwise undergo
 transplantation.
\end_layout

\begin_layout Standard
The research will involve analyzing data on past transplantations from the
 US-based 
\emph on
United Network for Organ Sharing (UNOS)
\emph default
 dataset, including information on both living and deceased donors, recipients
 and their post-transplant outcome.
 This data will be used to train and test machine learning models that can
 predict the patient's chances of surviving over time, given that he recieves
 a graft from a particular donor.
 These can then be compared in order to decide which donor-recipient pairing
 are the most compatible.
 This paper will focus particularyon the Random Survival Forests model which
 has demonstrated a promising performance in related scientific papers 
\begin_inset CommandInset citation
LatexCommand cite
key "Using ML methods to predict kidney transplant survival"
literal "false"

\end_inset

 .
 Additionally, the research will also examine the performance of the developed
 models on different population samples, namely on the dataset provided
 by the Czech 
\emph on
Institute for Clinical and Experimental Medicine (IKEM)
\emph default
.
\end_layout

\begin_layout Standard
The results of this thesis will hopefully provide valuable insights into
 application of machine learning in the field of transplantation medicine.
 If the models developed in this research are found to be effective in predictin
g compatibility, it could lead to a better and more efficient matching of
 donors and recipients, resulting in risk reduction for the transplant patients
 as complications such as graft rejection would be less probable.
 Furthermore, this research will also provide a valuable contribution to
 the broader field of medical research, by demonstrating the potential of
 machine learning in improving the success rate of medical treatments and
 reducing the burden on the healthcare system.
\end_layout

\begin_layout Chapter
Machine Learning
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{headings}
\end_layout

\end_inset


\end_layout

\begin_layout Section
General Overview of Machine Learning
\end_layout

\begin_layout Standard
Machine learning is a rapidly growing field within the larger discipline
 of artificial intelligence, which is concerned with the development of
 algorithms and statistical models that enable computers to autonomously
 improve their performance on a given task.
 The key idea behind machine learning is to develop algorithms that can
 identify patterns and relationships in large amounts of data, and then
 use these algorithms to make predictions or decisions based on newly introduced
 data.
\end_layout

\begin_layout Standard
In general, machine learning is a great tool when solving problems that
 would conventionally require an insurmountable amount of programming or
 when working with large amounts of data from which one could not easily
 extract information or identify meaningful patterns.
\end_layout

\begin_layout Subsection
Classification of Machine Learning Models
\end_layout

\begin_layout Standard
Machine learning models can be classified based on several parameters including
 the type of data used in training the models and the way they handle new
 data.
\end_layout

\begin_layout Subsubsection
Classification by Data Type
\end_layout

\begin_layout Standard

\emph on
Supervised Learning
\emph default
 is a type of Machine Learning where models require a labeled dataset for
 their learning, which means that along with the data, the desired output
 needs to provided to the computer as well.
 Supervised learning can be further subdivided into Regression and Classificatio
n, based on whether the desired output is a continuous numerical value or
 a discrete one respectively.
 The examples of Supervised Learning models include: 
\emph on
Linear
\emph default
 and 
\emph on
Logistic Regression
\emph default
, 
\emph on
Random Forests
\emph default
, 
\emph on
Decision Trees
\emph default
, 
\emph on
Supporting Vector Machines
\emph default
 and 
\emph on
Neural Networks
\emph default
.
\end_layout

\begin_layout Standard
The opposite of Supervised Learning is 
\emph on
Unsupervised Learning
\emph default
, where models are trained on an unlabeled dataset with the aim of finding
 patterns in the given data or grouping data with similar characteristics.
 An example of Unsupervised Learning is 
\emph on
Clustering
\emph default
 which is used to find groups with shared characteristics, Association which
 is used to find relation between the input variables (also called predictors)
 in a given dataset, or 
\emph on
Dimensional reduction
\emph default
 which is used to simplify data in order to more easily extract information
 from it.
 
\end_layout

\begin_layout Standard
A combination of these, called 
\emph on
Semi-supervised Learning
\emph default
, uses a combination of both labeled and unlabeled data and first uses an
 Unsupervised Machine Learning model to first cluster the data in order
 to classify the unlabeled data and then uses a Supervised machine Learning
 model on this newly labeled data.
 This is advantageous to using a simple Supervised Machine Learning model
 as it allows working with larger amounts of data that would otherwise be
 unusable because of the lack of labeling, thus potentially resulting in
 higher accuracy of the model.
\end_layout

\begin_layout Standard

\emph on
Reinforcement Learning
\emph default
 is a specialzed case of Semi-supervised learning, where the model is trained
 using feedback from the environment and is often used in cases where no
 labeled data exists or when the labeled dataset does not provide the best
 course of action.
 The learning system (called 
\emph on
agent
\emph default
 in this case) learns by performing actions from which it receives either
 rewards or penalties from the environment and based on these has to develop
 a strategy to maximize rewards (called 
\emph on
policy
\emph default
) 
\begin_inset CommandInset citation
LatexCommand cite
key "100-page ML,Hands-On ML"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Instance-based vs.
 Model-based
\end_layout

\begin_layout Standard

\emph on
Instance-based
\emph default
 learning algorithms work by comparing the similarity of new input data
 to the training data, an example of this is the 
\emph on
k-Nearest Neighbors
\emph default
 algorithm, which finds 
\begin_inset Formula $k$
\end_inset

 examples from the training data that have the most similar features to
 the given input and outputs either the most frequent or the average label
 value in this cohort.
 
\end_layout

\begin_layout Standard
On the other hand 
\emph on
Model-based learning
\emph default
 algorithms develop a mathematical function whose parameters are learned
 from the training data and predictions about new data are then accomplished
 by providing the newly introduced data as an input to the function.
 Model-based learning algorithms are for example 
\emph on
Linear Regression
\emph default
 and 
\emph on
Neural Networks
\emph default
.
\end_layout

\begin_layout Section
Training and Evaluating a Machine Learning Model
\end_layout

\begin_layout Subsection
Training, Validation and Test sets
\end_layout

\begin_layout Standard
The goal of machine learning is to create models that are able to make predictio
ns when faced with new data that the model hasn't seen during the training
 process.
 To achieve this, it is neccessary to split the data into 3 parts: 
\emph on
Training set
\emph default
, 
\emph on
Validation set 
\emph default
and
\emph on
 Test set.
 
\emph default
If a model is trained using all available data, it may perform well on said
 data, but may be unable to generalise for new instances thus making it
 useless in practice, that's why it's important to keep some of the data
 aside for validation and testing, these two together are called 
\emph on
hold-out 
\emph default
sets 
\begin_inset CommandInset citation
LatexCommand cite
key "100-page ML"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
It is desirable to keep the majority of the data for testing with the usual
 distribution being 70% for training and 15% for training and validation
 each, though with larger datasets, it is possible to allocate even higher
 percentage of the data to training.
\end_layout

\begin_layout Standard
Once the training of the model has been done using the training set, the
 performance of the model is tested using the test set, i.e.
 data that it hasn't seen before.
 A good metric for measuring the quality of the model is the 
\emph on
generalization error, 
\emph default
given by the error rate of the model on new cases.
\end_layout

\begin_layout Standard
Better performace of the model can be achieved using a validation set, which
 gives the option to select the best values for the model's hyperparameters
 (specifics of a given machine learning model that are set before the training
 process begins).
 This is done by training the model multiple times on the training set with
 different hyperparametrs and then comparing their perfomance on the validation
 set.
 Once the best model has been selected, it is then trained using both the
 training and the validation set and its performance is then measured using
 the test set 
\begin_inset CommandInset citation
LatexCommand cite
key "Hands-On ML"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Overfitting and Underfitting
\end_layout

\begin_layout Standard
If the generalization error of the model is low on the training set, but
 high on the test set, it means that the model has learned unnecessary details
 from the training set which then hampers its ability to generalise for
 new instances.
 This phenomena is called 
\emph on
overfitting 
\emph default
and is a common problem that arrises during the process of developing a
 machine learning model.
 Overfitting usually caused either by the data being too noisy (errors in
 the data, many outliers), the dataset being too small or by having too
 many irrelevant features.
 One way to overcome this is too adressed the above mentioned problems by
 gathering more data, removing outliers and inputing errors in the data
 or by simplifying the model by choosing fewer features.
 Another way of solving the issue of overfitting is constraining how much
 the model can change the values of its parameters, thus making the model
 simpler and less prone to overfitting.
 This is called 
\emph on
regularization 
\begin_inset CommandInset citation
LatexCommand cite
key "Hands-On ML"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Machine learning models can also face an opposite problem, i.e.
 not being able learn the underlying patterns in the data and thus being
 inaccurate on both the test and training data.
 This is called 
\emph on
underfitting 
\emph default
and can be solved by using a more appropriate (more complex) model for the
 given task, better selecting features to train the model on or reducing
 any constraints that might have set to simplify the model in order to prevent
 overfitting 
\begin_inset CommandInset citation
LatexCommand cite
key "Hands-On ML"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Decision Trees and Random Forests
\end_layout

\begin_layout Subsection
Decision Trees
\begin_inset CommandInset label
LatexCommand label
name "subsec:Decision-Trees"

\end_inset


\end_layout

\begin_layout Standard
Decision Trees are a supervised, model-based machine learning method used
 for both regression and classification, whose main benefits are that it
 can handle complex and nonlinear relations in data.
 There are two types of decision trees based on the type of target variable:
 
\emph on
Classification trees 
\emph default
and 
\emph on
Regression trees.
 
\end_layout

\begin_layout Standard
Decision trees are in practice mostly binary trees where in each parent
 node (also called 
\emph on
decision node
\emph default
), a specific attribute of the feature vector 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Decide on terminology: feature vector / predictor vector
\end_layout

\end_inset

 is examined and a split is made based on a given criteria whose specifics
 are learned during the training process.
 For example if a value of the given attribute is below a specific threshold,
 the left branch is followed, otherwise the right branch is followed, with
 the threshold being set to maximize a certain performance metric of the
 model.
 Once the leaf node (also called 
\emph on
terminal node
\emph default
) is reached, the example is assigned a probability of belonging to a given
 categorical value in case of a classification tree or assigned a real target
 value in case of a regression tree.
 
\end_layout

\begin_layout Standard
Different decision tree algorithms use different split rules also known
 as 
\emph on
criterions.

\emph default
 For example, one of the most common algorithms used for generating a classifica
tion tree is called ID3, which selects what attributes from the feature
 vector to split upon based on 
\emph on
Entropy
\emph default
 or 
\emph on
Information gain
\emph default
 of the subsets created by the split made on each feature.
 In the case of Regression Trees, the criterion used is often based on the
 reduction of 
\emph on
variance
\emph default
 or 
\emph on
standard deviation
\emph default
 between labels and their mean values.
 Another type of criterion is used by the decision trees in Random Survival
 Forests, described in more detail later, which use a log-rank test statistic
 in order to maximize the difference between the predicted survival time
 in each leaf node.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Building a Classification Tree
\end_layout

\begin_layout Standard
A classification tree using the ID3 algorithm is built as follows 
\begin_inset CommandInset citation
LatexCommand cite
key "100-page ML"
literal "false"

\end_inset

:
\begin_inset Newline newline
\end_inset

Let 
\begin_inset Formula $C$
\end_inset

=
\begin_inset Formula $\left\{ 1,2,\ldots,p\right\} $
\end_inset

 be the set of possible categorical values, 
\begin_inset Formula $\left\{ \left(x^{(i)},y^{(i)}\right)\right\} _{i=1}^{N}$
\end_inset

is a collection of labeled examples with numerical predictors, where 
\begin_inset Formula $N$
\end_inset

 is the size of the collection, 
\begin_inset Formula $\mathbf{x}^{(i)}=(x_{1}^{(i)},\ldots,x_{k}^{(i)})$
\end_inset

 is a 
\begin_inset Formula $k$
\end_inset

-dimensional feature vector of the example 
\begin_inset Formula $i\in\hat{N}$
\end_inset

 and 
\begin_inset Formula $y^{(i)}\in C$
\end_inset

 is its label.
 The decision tree model is denoted by
\begin_inset Formula $f(\mathbf{x})$
\end_inset

 which is a function that estimates the probability of a given example to
 be of class 
\begin_inset Formula $a\in C$
\end_inset

 i.e.
 it is defined as
\begin_inset Formula 
\begin{equation}
f(\mathbf{x},a)\stackrel{\mathrm{def}}{=}\Pr\left(y=a\mid\mathbf{x}\right),\label{eq:class tree def}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is a 
\begin_inset Formula $k$
\end_inset

-dimensional feature vector and 
\begin_inset Formula $y$
\end_inset

 is a random variable describing the class of a given example.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $S_{i}$
\end_inset

 denote the set of labeled examples included in a given node 
\begin_inset Formula $i$
\end_inset

 , 
\begin_inset Formula $p^{S_{i}}(a)$
\end_inset

 denote the proportion of the examples in 
\begin_inset Formula $S_{i}$
\end_inset

 that belong to class 
\begin_inset Formula $a$
\end_inset

 and let 
\begin_inset Formula $F_{i}$
\end_inset

 denote the set of all possible features in a given node 
\begin_inset Formula $i$
\end_inset

.
 In the first step of the algorithm, the decision tree is consists of only
 its root node that contains all of the labeled examples i.e.
 
\begin_inset Formula $S_{0}=$
\end_inset

 
\begin_inset Formula $\left\{ \left(x^{(i)},y^{(i)}\right)\right\} _{i=1}^{N}$
\end_inset

, all features are available i.e.
 
\begin_inset Formula $F_{0}=\hat{k}$
\end_inset

 and the proportion is given by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p^{S_{0}}\left(a\right)=\frac{\left|\left\{ \left(\mathbf{x}^{(i)},y^{(i)}\right)\mid y^{(i)}=a,\left(\mathbf{x}^{(i)},y^{(i)}\right)\in S_{0}\right\} \right|}{\left|S_{0}\right|}.\label{eq:class tree proportion}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The next step of the algorithm is to iterate over all possible features
 
\begin_inset Formula $j\in F_{0}$
\end_inset

 and all possible threshold values 
\begin_inset Formula $t$
\end_inset

 (the possible threshold values can be chosen, for example, as midpoints
 between values of the predictors e.g.
 if the predictor 
\begin_inset Formula $j$
\end_inset

 takes on the values 
\begin_inset Formula $\left[1,2,3\right]$
\end_inset

 then we may choose the possible threshold values as 
\begin_inset Formula $\left[1.5,2.5\right]$
\end_inset

) and split 
\begin_inset Formula $S_{0}$
\end_inset

 into two subsets defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
S_{0-}\stackrel{\mathrm{def}}{=}\left\{ \left(\mathbf{x},y\right)\mid\left(\mathbf{x},y\right)\in S_{0},x_{j}<t\right\} \quad\mathrm{and}\quad S_{0+}\stackrel{\mathrm{def}}{=}\left\{ \left(\mathbf{x},y\right)\mid\left(\mathbf{x},y\right)\in S_{0},x_{j}\geq t\right\} .\label{eq:class tree split}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
For each iteration (ordered pair 
\begin_inset Formula $\left(j,t\right)$
\end_inset

), the quality of the split is calculated by using the algorithm's criterion.
 In the case of the ID3 algorithm, the criterion is the entropy of a given
 set of examples which is given by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
H(S)=-\sum_{a\in S}p^{S}\left(a\right)\log_{2}p^{S}\left(a\right).\label{eq:entropy}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The quality of the given split is then determined by the weighted sum of
 the entropies of the two subsets created by the split i.e.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
H(S_{-},S_{+})=\frac{\left|S_{-}\right|}{\left|S\right|}H(S_{-})+\frac{\left|S_{+}\right|}{\left|S\right|}H(S_{+}),\label{eq:class tree entropy}
\end{equation}

\end_inset

where the goal is to minimize 
\begin_inset Formula $H(S_{-},S_{+})$
\end_inset

.
\end_layout

\begin_layout Standard
Once the best split has been found, each of the created subsets serves as
 a new decision node, i.e.
 
\begin_inset Newline newline
\end_inset

 
\begin_inset Formula $S_{1}\coloneqq S_{0-}$
\end_inset

, 
\begin_inset Formula $S_{2}:=$
\end_inset


\begin_inset Formula $S_{0+}$
\end_inset

 and the branching continues considering only the attributes that were not
 previously used, i.e.
 given that 
\begin_inset Formula $j_{0}\in F_{0}$
\end_inset

 was chosen as the optimal predictor to split upon, then 
\begin_inset Formula $F_{1}=F_{2}=F_{0}\setminus\left\{ j_{0}\right\} $
\end_inset


\end_layout

\begin_layout Standard
The algorithm stops if either there are no further attributes to split upon,
 all possible decisions would reduce entropy less then some set amount or
 the tree reaches a set maximum depth (the minimum number of edges connecting
 the root to a leaf node).
\end_layout

\begin_layout Standard
When a new input 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is introduced in the form of a 
\begin_inset Formula $k-$
\end_inset

dimensional vector whose attributes are of the same data type as in the
 feature vectors 
\begin_inset Formula $\mathbf{x}^{(i)}$
\end_inset

, the decision tree is followed from the root down, evaluating the attributes
 of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in each decision node until a leaf node is reached.
 Let 
\begin_inset Formula $S_{n}$
\end_inset

 denote such leaf node, then for 
\begin_inset Formula $\forall a\in C$
\end_inset


\begin_inset Formula 
\begin{equation}
f(\mathbf{x},a)=p^{S_{n}}\left(a\right)=\frac{\left|\left\{ \left(\mathbf{x}^{(i)},y^{(i)}\right)\in S_{n}\mid y^{(i)}=a\right\} \right|}{\left|S_{n}\right|}.\label{eq:class tree final}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
Building a Regression Tree
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
https://www.saedsayad.com/decision_tree_reg.htm#:~:text=The%20ID3%20algorithm%20can
%20be,Gain%20with%20Standard%20Deviation%20Reduction.&text=A%20decision%20tree%20
is%20built,with%20similar%20values%20(homogenous).
\end_layout

\begin_layout Plain Layout
https://www.datascienceprophet.com/understanding-the-mathematics-behind-decision-t
ree-algorithm-part-ii/
\end_layout

\begin_layout Plain Layout
https://medium.com/machine-learning-researcher/decision-tree-algorithm-in-machine
-learning-248fb7de819e
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This time suppose the opposite case i.e.
 that the dataset consists of examples with categorical valued predictors
 and labels with continuous numerical values.
 As a result, the Standard Deviation Reduction algorithm would be good choice
 to build the regression tree.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X_{j}$
\end_inset

 be the set of all possible categorical values for the feature 
\begin_inset Formula $j\in\hat{k},$
\end_inset

where 
\begin_inset Formula $k$
\end_inset

 is the total number of features in the dataset.
 Again let 
\begin_inset Formula $\left\{ \left(x^{(i)},y^{(i)}\right)\right\} _{i=1}^{N}$
\end_inset

 denote a collection of labeled examples, where 
\begin_inset Formula $N$
\end_inset

 is the size of the collection, 
\begin_inset Formula $\mathbf{x}^{(i)}=(x_{1}^{(i)},\ldots,x_{k}^{(i)})$
\end_inset

 is a 
\begin_inset Formula $k$
\end_inset

-dimensional feature vector of the example 
\begin_inset Formula $i\in\hat{N}$
\end_inset

, where 
\begin_inset Formula $x_{j}^{(i)}\in X_{j}$
\end_inset

 for 
\begin_inset Formula $\forall j\in\hat{k}$
\end_inset

 and 
\begin_inset Formula $y^{(i)}\in\mathbb{R}^{+}$
\end_inset

 is the target value.
 The decision tree model will this time be a function that takes a 
\begin_inset Formula $k$
\end_inset

-dimensional feature vector with the same type of attributes as 
\begin_inset Formula $\mathbf{x}^{(i)}$
\end_inset

, 
\begin_inset Formula $i\in\hat{k}$
\end_inset

 as input, i.e.
 if 
\begin_inset Formula $\mathbf{x=}(x_{1},\ldots,x_{k})$
\end_inset

 then 
\begin_inset Formula $x_{j}\in X_{j}$
\end_inset

 for 
\begin_inset Formula $\forall j\in\hat{k}$
\end_inset

 and outputs a real valued estimation, i.e.
 
\begin_inset Formula $f(\mathbf{x})\in\mathbb{R}^{+}$
\end_inset

.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $S_{i}$
\end_inset

 denote the set of labeled examples included in a given node 
\begin_inset Formula $i$
\end_inset

, 
\begin_inset Formula $F_{i}$
\end_inset

 is the set of all possible features in the node 
\begin_inset Formula $S_{i}$
\end_inset

, 
\begin_inset Formula $\sigma(S_{i})$
\end_inset

 is the standard deviation for labels in 
\begin_inset Formula $S_{i}$
\end_inset

, while 
\begin_inset Formula $\sigma\left(S_{i},j\right)$
\end_inset

 is standard deviation in 
\begin_inset Formula $S_{i}$
\end_inset

 for feature 
\begin_inset Formula $j\in F_{i}$
\end_inset

.
\end_layout

\begin_layout Standard
In the first step of the algorithm, again the decision tree consists of
 only its root node that contains all labeled example i.e.
 
\begin_inset Formula $S_{0}=$
\end_inset

 
\begin_inset Formula $\left\{ \left(x^{(i)},y^{(i)}\right)\right\} _{i=1}^{N}$
\end_inset

, 
\begin_inset Formula $F_{0}=\hat{k}$
\end_inset

.
 The standard deviation of all labels in 
\begin_inset Formula $S_{0}$
\end_inset

 given by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\sigma\left(S_{0}\right)=\sqrt{\frac{1}{N}\sum_{i=1}^{N}\left(y^{(i)}-\overline{y}\right)^{2}},
\end{equation}

\end_inset

where 
\begin_inset Formula $N$
\end_inset

 is the number of examples in 
\begin_inset Formula $S_{0}$
\end_inset

 and 
\begin_inset Formula $\overline{y}$
\end_inset

 is the mean value of labels in 
\begin_inset Formula $S_{0}$
\end_inset

.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $\forall j\in F_{0}$
\end_inset

, the standard deviation for the feature 
\begin_inset Formula $j$
\end_inset

 is then calculated by
\begin_inset Formula 
\begin{equation}
\sigma\left(S_{0},j\right)=\sum_{c\in X_{j}}p\left(c\right)\sigma\left(c\right)
\end{equation}

\end_inset

where 
\begin_inset Formula $p\left(c\right)$
\end_inset

 is the proportion of examples in 
\begin_inset Formula $S_{0}$
\end_inset

 whose feature 
\begin_inset Formula $j$
\end_inset

 belongs to the class 
\begin_inset Formula $c$
\end_inset

.
 Let 
\begin_inset Formula $N_{c}$
\end_inset

 denote the number of examples in 
\begin_inset Formula $S_{0}$
\end_inset

 whose feature 
\begin_inset Formula $j$
\end_inset

 belongs to the class 
\begin_inset Formula $c$
\end_inset

 i.e.
 
\begin_inset Formula $N_{c}=|\left\{ \left(\mathbf{x}^{(i)},y^{(i)}\right)\in S_{0}\mid x_{j}^{(i)}=c\right\} |$
\end_inset

, then 
\begin_inset Formula $p\left(c\right)$
\end_inset

 is given by
\begin_inset Formula 
\begin{equation}
p\left(c\right)=\frac{N_{c}}{\left|S_{0}\right|}
\end{equation}

\end_inset

and 
\begin_inset Formula $\sigma\left(c\right)$
\end_inset

 is the standard deviation of labels in 
\begin_inset Formula $S_{0}$
\end_inset

 whose feature 
\begin_inset Formula $j$
\end_inset

 belongs to the class c, i.e.
\begin_inset Formula 
\begin{equation}
\sigma\left(c\right)=\sqrt{\frac{1}{N_{c}}\sum_{i=1}^{N_{c}}\left(y_{c}^{(i)}-\overline{y_{c}}\right)^{2}}
\end{equation}

\end_inset

where 
\begin_inset Formula $y_{c}^{(i)}$
\end_inset

 are the labels of examples whose feature 
\begin_inset Formula $j$
\end_inset

 belongs to the class 
\begin_inset Formula $c$
\end_inset

 and 
\begin_inset Formula $\overline{y_{c}}$
\end_inset

 is their mean value.
\end_layout

\begin_layout Standard
The best split is then chosen as the one with the highest 
\emph on
standard deviation reduction
\emph default
 given by
\begin_inset Formula 
\begin{equation}
\varDelta\sigma\left(S_{0},j\right)=\sigma\left(S_{0}\right)-\sigma\left(S_{0},j\right).
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $S_{0}$
\end_inset

 is then split into subsets based on the chosen feature 
\begin_inset Formula $j$
\end_inset

 (the split is not necessarily binary), i.e.
\begin_inset Formula 
\begin{equation}
S_{c}=\left\{ \left(x^{(i)},y^{(i)}\right)\in S_{0}\mid x_{j}^{(i)}=c\right\} \qquad\forall c\in X_{j}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where each 
\begin_inset Formula $S_{c}$
\end_inset

 then serves as a new decision node and the process repeats.
\end_layout

\begin_layout Standard
The algorithm is stopped if either the selected reduction in variation is
 below a set threshold, there are no more features to split upon or if the
 tree reaches set maximum depth.
 
\end_layout

\begin_layout Standard
When the model is presented with a new input 
\begin_inset Formula $\mathbf{x}$
\end_inset

, it follows the tree from the root down until a leaf node is reached.
 Let 
\begin_inset Formula $S_{n}$
\end_inset

 denote such a leaf node , then output values is given by the average value
 of the labels in 
\begin_inset Formula $S_{n}$
\end_inset

, i.e.
\begin_inset Formula 
\begin{equation}
f\left(\mathbf{x}\right)=\frac{1}{\left|S_{n}\right|}\sum_{(\mathbf{x}^{(i)},y^{(i)})\in S_{n}}y^{(i)}.
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Ensemble Learning and Random Forests
\end_layout

\begin_layout Standard

\emph on
Ensemble learning 
\emph default
is machine learning technique that combines the predictions of multiple
 models in order to boost the overall performance of a model on a given
 task.
 An example of an ensemble learning method are 
\emph on
random forests
\emph default
, which build on the foundation of decision trees by aggregating the predictions
 made by a set amount of decision trees, allowing the model to make more
 accurate predictions than any single decision tree could make by itself.
 The key idea behind random forests is that every decision tree is trained
 using a different sample of input features, which helps reduce overfitting
 and improves the generalization performance of the model.
\end_layout

\begin_layout Standard
An example of a random forests model is training a group of classification
 trees on different random subsets of the training data and when faced with
 new input, predicting the class of the given input by choosing the class
 that got the most 
\begin_inset Quotes sld
\end_inset

votes
\begin_inset Quotes srd
\end_inset

 by the classification trees 
\begin_inset CommandInset citation
LatexCommand cite
key "Hands-On ML"
literal "false"

\end_inset

.
 A class 
\begin_inset Formula $c$
\end_inset

 gets a 
\begin_inset Quotes sld
\end_inset

vote
\begin_inset Quotes srd
\end_inset

 by the decision tree when the input 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is assigned the highest probability of belonging to 
\begin_inset Formula $c$
\end_inset

, i.e.
 class 
\begin_inset Formula $c\in C$
\end_inset

 gets the vote 
\begin_inset Formula $\Longleftrightarrow c=\underset{a\in C}{\arg\max}$
\end_inset


\begin_inset Formula $\Pr\left(y=a\mid\mathbf{x}\right)$
\end_inset

 , where 
\begin_inset Formula $C$
\end_inset

 is the set of all possible classes.
\end_layout

\begin_layout Subsubsection
Building a Random Forest
\end_layout

\begin_layout Standard
The process of building a random forest model can divided into four steps
 
\begin_inset CommandInset citation
LatexCommand cite
key "100-page ML,Hands-On ML"
literal "false"

\end_inset

:
\end_layout

\begin_layout Enumerate

\emph on
Sampling the data: 
\emph default
In order to boost the performance of a random forests model (and other ensemble
 learning methods), it is vital to train the decision trees (or other predictors
) 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Clash in terminology: here predictors refers to the models themselves while
 before it referred to the attributes/ features of the data.
\end_layout

\end_inset

 on different samples of the data in order to minimize risk of the predictors
 making the same kind of error.
 Samples are taken randomly from the dataset in order to create multiple
 different subsets of the data.
 This process can be divided based on whether the samples are taken 
\begin_inset Quotes sld
\end_inset

with replacement
\begin_inset Quotes srd
\end_inset

 or not i.e.
 whether instances of the data can be used in training of multiple predictors
 or just once, this is called 
\emph on
bagging 
\emph default
(abbreviation of 
\emph on
bootstrap aggregating
\emph default
) and
\emph on
 pasting 
\emph default
respectively, with bagging being the generally more often used case for
 random forests.
\end_layout

\begin_layout Enumerate

\emph on
Feature sampling: 
\emph default
In order to even more differentiate the individual decision trees, only
 a random subset of the input features are considered when building a particular
 decision tree.
 The model will perform best when the input feature are independent of each
 other.
 
\end_layout

\begin_layout Enumerate

\emph on
Building the decision trees
\emph default
: For each sample of the data and input features, a decision tree is built
 using a given split criterion.
 For even more randomized trees it is possible to randomly select the threshold
 values for each split in the decision trees, instead of searching for the
 best possible threshold.
 These are called 
\emph on
Extremely Randomized Trees.
\end_layout

\begin_layout Enumerate

\emph on
Aggregating Predictions
\emph default
: Random forests make predictions by aggregating the predictions made by
 the indivudual decision trees, for example in the case of classification
 task, the class with the biggest amount of votes is selected while in the
 case of a regression task, the predicted value is calculated by averaging
 the values predicted by the individual decision trees.
\end_layout

\begin_layout Standard
The most important hyperparameters to be set before the beginning of the
 training process are the number of trees in the forest, the size of the
 bootstrap sample, the size of the feature sample, and the split criterion
 used to build the decision trees.
\end_layout

\begin_layout Chapter
Renal Transplantation
\end_layout

\begin_layout Section
Chronic Kidney Disease
\end_layout

\begin_layout Standard
The 
\emph on
chronic kidney disease 
\emph default
(CKD)
\emph on
 
\emph default
is a medical condition in which the kidneys gradually lose their ability
 to filter waste products and excess fluids from the blood system.
 The prevalence of CKD is estimated to be between 8% and 16% worldwide 
\begin_inset CommandInset citation
LatexCommand cite
key "CKD: Global Dimensions and Perspectives"
literal "false"

\end_inset

 and is observed to be on the rise, as in 2015, CKD caused roughly 
\begin_inset Formula $1.2$
\end_inset

 million deaths compared to 
\begin_inset Formula $409,000$
\end_inset

 in 1990.
 Symptoms of CKD include fatique, nausea, loss of apetite, leg swelling
 and itching.
 As the disease progresses, it may result in health complications such as
 anemia, cardiovascular disease, bone disease and nerve demage.
 In the final stages when the kidneys fail altogether, the affected person
 needs to regularly undergo dialysis, which supplements the kidney's functions.
 However this needs to be done several times a week and each session lasts
 between 3 to 5 hours, which leads in a significant decrease in quality
 of the patients life.
 Thus when possible, transplantation is a much preferred treatment for CKD.
\end_layout

\begin_layout Standard
CKD is the main cause for renal transplnatation worldwide as roughly 90%
 of all renal tranplantation are performed to treat the 
\emph on
end-stage renal disease
\emph default
, which is the final stage of CKD.
\end_layout

\begin_layout Section
Pre-Transplant Procedures
\end_layout

\begin_layout Standard
There are number of procedures that patients has to undergo before he is
 admitted onto the kindey transplantation wainting list.
\end_layout

\begin_layout Standard
Firstly, their overall health is checked in order evaluate whether they
 are suitable candidate for the procedure.
 This includes blood tests, imaging studies, and other diagnostic tests
 to check for any underlying health conditions that could affect the success
 of the transplant.
\end_layout

\begin_layout Standard
Secondly, a process called 
\emph on
HLA typing 
\emph default
is performed both on the patient and the recipient.
 The process involves testing the person's blood and tissue to determine
 their human leukocyte antigen (HLA) type, which is a protein that plays
 a key role in the body's immune response ( explained in more detail in
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:HLA-Typing"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Uncompatible pairs are discarded while potentionally compatible pairs have
 their blood tested against each other in a laboratory using a process called
 
\emph on
cross-matching.
\end_layout

\begin_layout Standard

\emph on
Cross-matching
\emph default
 involves testing the blood of the potential donor against the blood of
 the recipient to ensure that there are no antibodies that could cause a
 reaction.
 If the recipient's serum contains antibodies that react with the donor's
 cells, the complement will be activated and will cause the cells to burst,
 or lyse.
 This reaction can be seen under a microscope, and the degree of lysis can
 be graded to determine the strength of the reaction.
 When no reaction occurs, the cross-match is called 
\emph on
negative
\emph default
, whereas if a reaction occurs, the crossmatch is called 
\emph on
positive.
 
\emph default
A positive crossmatch doesn not automatically mean that the tranplantation
 cannot proceed though, as a succesful tranplantation may still be possible,
 given additional treatment, the pateint's health and if the reaction is
 not too severe.
 This is often the case when other donors are not easily available.
\end_layout

\begin_layout Subsection
HLA Typing 
\begin_inset CommandInset label
LatexCommand label
name "subsec:HLA-Typing"

\end_inset


\end_layout

\begin_layout Standard
TODO: Explain HLA typing
\end_layout

\begin_layout Chapter
Survival Analysis
\end_layout

\begin_layout Section
Introduction to Survival Analysis
\end_layout

\begin_layout Standard
Survival analysis is a collection of statistical procedures for data analysis
 for which the outcome variable of interest is time until an event occurs,
 usually referred to as 
\emph on
time-to-event 
\emph default
or 
\emph on
survival time.

\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "Survival Analysis self-learn"
literal "false"

\end_inset

.
 More specifically, time-to-event refers to the time from the beginning
 of a follow-up i.e.
 the period during which the subject of a study is observed, usually from
 the date of diagnosis, the start of a treatment or transplantation, until
 the occurrence of an event of interest, usually meaning the death of a
 patient, disease incidence, relapse from remission, recovery or in the
 case of kidney transplantation: 
\emph on
graft failure
\emph default
.
 Survival analysis involves several statistical techniques that are used
 during the pre-transplant procedure as a part of the process of finding
 compatible donor-recipient pairing, namely the 
\emph on
Kaplan-Meier estimator
\emph default
, the 
\emph on
Log-Rank test 
\emph default
and the 
\emph on
Cox proportional hazards model
\emph default
 just to name a few.
 These will be explained in detail further in this chapter.
\end_layout

\begin_layout Subsection
Data Censoring
\end_layout

\begin_layout Standard
A crucial challenge that all survival analysis methods have to tackle is
 
\emph on
data censoring
\emph default
, which refers to the lack of data about the exact survival time of an individua
l, which happens, for example, as a result of a study ending before the
 said individual experiences an event or if the individual withdraws from
 the study.
 This kind of censoring is called 
\emph on
right censoring, 
\emph default
meaning that if the survival time were to be plotted along a horizontal
 axis, the event would happen to the right side of the cut-off point of
 the study, but it is unkown by how much.
 
\end_layout

\begin_layout Standard

\emph on
Left censored 
\emph default
data are also possible, but they are uncommon in the case of post-tranplantation
 data as the the date of tranplantation is naturally almost always known.
 Left censoring can happen, for example, in the case of predicting the survival
 time after exposure to a virus, as the exact date of this occurence is
 often unkown.
\end_layout

\begin_layout Subsection
Survival and Hazard Functions
\end_layout

\begin_layout Standard
The two fundamental functions of survival analysis are the 
\emph on
survival function 
\emph default
denoted by 
\begin_inset Formula $S(t)$
\end_inset

 and the 
\emph on
hazard function, 
\emph default
also called the 
\emph on
instantaneous failure rate 
\emph default
denoted by 
\emph on

\begin_inset Formula $h(t)$
\end_inset

.
 
\emph default
The survival function describes the probability of an individual 
\emph on
surviving
\emph default
 (i.e.
 not experiencing an event) longer, than a specified point in time 
\begin_inset Formula $t$
\end_inset

.
 In another words, it is definded as
\begin_inset Formula 
\begin{equation}
S(t)=P(T>t)
\end{equation}

\end_inset

where 
\begin_inset Formula $T$
\end_inset

 is the random variable the survival time of an individual.
 
\end_layout

\begin_layout Standard
In contrast, the hazard function, as hinted by its alternative name, gives
 the the instantaneous potential per unit time for the event to occur, given
 that the individual has survived up to time t, that is the hazard function
 is focused 
\emph on
failing
\emph default
 (i.e.
 event occuring) compared to the survival function which is focused on 
\emph on
not failing 
\emph default
(i.e.
 surviving) 
\begin_inset CommandInset citation
LatexCommand cite
key "Survival Analysis self-learn"
literal "false"

\end_inset

.
 What is meant by the word 
\emph on
potential
\emph default
, is the probability of an event happening within a given time frame and
 as this this potential is 
\emph on
instantenous
\emph default
, it is given by the following limit
\begin_inset Formula 
\begin{equation}
h(t)=\lim_{\Delta t\to0}\frac{P\left(t\leq T<t+\Delta t\mid T\geq t\right)}{\Delta t}.
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
While the survival functions may seem to be obvious option for describing
 models aimed at predicting survival time, it is usually the hazard function,
 with which the survival models are described by, since it provides a more
 direct measure of risk over time.
 Importantly, either of the function can be derived from the other one using
 the following formulae´
\begin_inset Formula 
\begin{equation}
S(t)=\exp\left[-\intop_{0}^{t}h(u)du\right]
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
h(t)=-\left(\frac{\frac{dS(t)}{dt}}{S(t)}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Maybe write a paragraph about 
\emph on
interaction 
\emph default
and 
\emph on
confounding
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Hazard Ratio
\end_layout

\begin_layout Standard
One of the most common goals of survival analysis is to describe a correlation
 between a given 
\emph on
exposure variable
\emph default
 ( i.e.
 a feature of an instance in the data) and the 
\emph on
outcome variable
\emph default
 (e.g.
 survival time).
 This relationship is described using the 
\emph on
hazard ratio
\emph default
 (HR), which describes the relative risk of an event occurring in one group
 compared to another , i.e.
 it is calculated as the ratio of the hazard functions of the two groups
\begin_inset Formula 
\begin{equation}
\mathrm{HR}(t)=\frac{h_{1}(t)}{h_{2}(t)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
A hazard ratio of 1, means that the probability of an event occuring at
 a given time is the same in both groups.
 If, for example, said groups are sorted by the value of a certain exposure
 variable and their hazard ratio is 1, this means that there is no relationship
 between said exposure variable and the outcome variable.
\end_layout

\begin_layout Standard
Analogously, a hazard ratio of 5 would mean that the first group has a five
 times larger hazard than the second group 
\begin_inset CommandInset citation
LatexCommand cite
key "Survival Analysis self-learn"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Survival Analysis Methods
\end_layout

\begin_layout Subsection
Kaplan-Meier Estimator 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Kaplan-Meier-Estimator"

\end_inset


\end_layout

\begin_layout Standard
One of the most widely used methods to estimate the survival function is
 the 
\emph on
Kaplan-Meier estimator 
\emph default
that works as follows.
 Let there be a dataset containing 
\begin_inset Formula $n$
\end_inset

 entries with each entry 
\begin_inset Formula $i$
\end_inset

 containing information about the individual's survival time 
\begin_inset Formula $T_{i}$
\end_inset

 and censoring status 
\begin_inset Formula $\delta_{i}$
\end_inset

, where 
\begin_inset Formula $\delta_{i}=1$
\end_inset

 if an event has been observed at time 
\begin_inset Formula $T_{i}$
\end_inset

 and 
\begin_inset Formula $\delta_{i}=0$
\end_inset

 if the entry is right censored.
 Let 
\begin_inset Formula $\tau$
\end_inset

 denote the set of all unique event times, i.e 
\begin_inset Formula $\tau\coloneqq\left\{ T_{i}\mid\delta_{i}=1,\:i=0,\ldots,n\right\} ,$
\end_inset

 let 
\begin_inset Formula $f_{j}$
\end_inset

 denote the denote the number of individuals who fail at time 
\begin_inset Formula $t_{j}$
\end_inset

 and let 
\begin_inset Formula $r_{j}$
\end_inset

 denote the number of individuas at risk at time 
\begin_inset Formula $t_{j}$
\end_inset

 (i.e.
 have yet to experience an event or are yet to be censored), then 
\begin_inset Formula $\forall t_{j}\in\tau$
\end_inset

 the probability of an individual surving past 
\begin_inset Formula $t_{j}$
\end_inset

 for 
\begin_inset Formula $j=1,\ldots,|\tau|$
\end_inset

 given that they have already survived until at least 
\begin_inset Formula $t_{j}$
\end_inset

 is given by the conditional probability 
\begin_inset Formula $\Pr(T>t_{j}\mid T\geq t_{j})$
\end_inset

 that is deductible from the dataset as follows
\begin_inset Formula 
\begin{equation}
\Pr(T>t_{j}\mid T\geq t_{j})=1-\frac{f_{j}}{r_{j}}.
\end{equation}

\end_inset

Note that the number of individuals at risk does not take in account the
 individuals that were previously censored, that is 
\begin_inset Formula $r_{j}=r_{j-1}-f_{j-1}-c_{j-1}$
\end_inset

, where 
\begin_inset Formula $c_{j-1}$
\end_inset

is the number of individuals that were censored during 
\begin_inset Formula $[t_{j-1},t_{j})$
\end_inset

.
 
\end_layout

\begin_layout Standard
The survival function is a function that outputs the probability of surviving
 past a given time 
\begin_inset Formula $t$
\end_inset

, therefore, thanks to the 
\emph on
chain rule
\emph default
 of probability, the 
\emph on
Kaplan-Meier curve
\emph default
 
\begin_inset Formula $\hat{S}$
\end_inset

 estimates the survival function at times 
\begin_inset Formula $t_{1},\ldots,t_{|\tau|}$
\end_inset

 as follows 
\begin_inset CommandInset citation
LatexCommand cite
key "Survival Analysis self-learn"
literal "false"

\end_inset


\begin_inset Formula 
\begin{equation}
\hat{S}(t_{j})=\Pr(T>t_{j})=\prod_{k=1}^{j}\Pr(T>t_{k}\mid T\geq t_{k}).
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Thanks to this notation, the Kaplan-Meier formula is often referred to as
 the 
\emph on
product-limit formula 
\emph default
as it is a product of probabilities that is limited by the time point being
 observed.
 The Kaplan-Meier curve itself assumes that 
\begin_inset Formula $\hat{S}(t)$
\end_inset

 remains constant over 
\begin_inset Formula $t_{j}\leq t<t_{j+1}$
\end_inset

, therefore the curve has a 
\begin_inset Quotes sld
\end_inset

stair-like
\begin_inset Quotes srd
\end_inset

 shape.
\end_layout

\begin_layout Subsection
Log-rank Test
\begin_inset CommandInset label
LatexCommand label
name "subsec:Log-rank-Test"

\end_inset


\end_layout

\begin_layout Standard
The log-rank test is statistical test used to compare the survival distribution
 of two or more groups and decide whether their survival curves are statisticall
y equivalent or not 
\begin_inset CommandInset citation
LatexCommand cite
key "Survival Analysis self-learn"
literal "false"

\end_inset

.
 The log-rank statistic is also used as a split-rule criterion for building
 survival trees that are explained in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Random-Survival-Forests"
plural "false"
caps "false"
noprefix "false"

\end_inset

, where it is used to compare the difference in survival times observed
 between the two groups that are created by a possible split in a given
 node of a survival tree.
 Therefore, the following explanation of the log-rank statistic is only
 concerned with comparing two groups.
\end_layout

\begin_layout Standard
Assuming that a dataset contains information about two groups of individuals
 along with their survival time and cesnoring status, let 
\begin_inset Formula $\tau$
\end_inset

 again denote the set of all unique event times regardless of group.
 Then 
\begin_inset Formula $\forall t_{j}\in\tau$
\end_inset

 an expected count of failures 
\begin_inset Formula $e_{j}^{(i)}$
\end_inset

 is computed for each group
\begin_inset Formula 
\begin{equation}
e_{j}^{(1)}=\frac{r_{j}^{(1)}}{r_{j}^{(1)}+r_{j}^{(2)}}\cdot(f_{j}^{(1)}+f_{j}^{(2)}),\qquad e_{j}^{(2)}=\frac{r_{j}^{(2)}}{r_{j}^{(1)}+r_{j}^{(2)}}\cdot(f_{j}^{(1)}+f_{j}^{(2)})
\end{equation}

\end_inset

where 
\begin_inset Formula $r_{j}^{(i)}$
\end_inset

 is the number of individuals from group 
\begin_inset Formula $i$
\end_inset

 at risk at time 
\begin_inset Formula $t_{j}$
\end_inset

 and 
\begin_inset Formula $f_{j}^{(i)}$
\end_inset

 is the number of failures in group 
\begin_inset Formula $i$
\end_inset

 at time 
\begin_inset Formula $t_{j}.$
\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $O_{i}-E_{i}=\sum_{t_{j}\in\tau}f_{j}^{(i)}-e_{j}^{(i)}$
\end_inset

 represent the sum of the differences between the observed and the expected
 counts of failures over all of the unique event times for group 
\begin_inset Formula $i$
\end_inset

.
 Then the log-rank statistic for the group 
\begin_inset Formula $i$
\end_inset

 is given by 
\begin_inset Formula 
\begin{equation}
L^{(i)}=\frac{(O_{i}-E_{i})^{2}}{\mathrm{Var}(O_{i}-E_{i})}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Cox Regression 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Cox-Regression"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Refine this section using 
\begin_inset CommandInset citation
LatexCommand cite
key "Survival Analysis self-learn"
literal "false"

\end_inset

 and add citatio
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\emph on
Cox regression
\emph default
, also known as the 
\emph on
Proportional hazards model
\emph default
 or 
\emph on
Cox proportional hazards model
\emph default
, is a type of survival model that is used to analyze the relationship between
 the time before some event happens and one or more predictor variables.
 It is commonly used in medical research to investigate the factors that
 influence the time to an event, such as graft failure.
\end_layout

\begin_layout Standard
The Cox regression model assumes that the 
\emph on
hazard rate
\emph default
 (the risk of the event occurring at any given time) is proportional across
 different levels of the predictor variables.
 In other words, the effect of the predictor variables on the hazard rate
 remains constant over time.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
The Cox regression model estimates the hazard ratio, which is the ratio
 of the hazard rate of one group compared to another group, while controlling
 for other factors.
 This allows researchers to examine the impact of different factors on the
 hazard rate while adjusting for potential confounding variables.
\end_layout

\begin_layout Plain Layout
Overall, Cox regression is a useful statistical technique that allows researcher
s to investigate the relationship between predictor variables and survival
 time, while taking into account the complex interplay between different
 factors.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Cox regression can be used to identify compatible pairs of donors and recipients
 for kidney transplantations by analyzing the survival time of transplanted
 kidneys and identifying factors that impact the likelihood of graft failure.
\end_layout

\begin_layout Standard
One important predictor variable in this context is the degree of human
 leukocyte antigen (HLA) matching between the donor and recipient.
 HLA is a group of genes that play a critical role in the immune system's
 ability to recognize and respond to foreign tissue, and HLA mismatches
 between donor and recipient can increase the risk of graft failure.
\end_layout

\begin_layout Standard
Using Cox regression allows analyzing the survival time of transplanted
 kidneys while controlling for other factors that may influence graft survival,
 such as age, sex, and underlying medical conditions and using the results
 to estimate the hazard ratio for HLA mismatch, which represents the relative
 risk of graft failure for each additional HLA mismatch.
 This information can then be used to identify compatible pairs of donors
 and recipients by selecting those with the lowest risk of graft failure
 based on their HLA compatibility and other relevant factors.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Overall, Cox regression is a powerful tool for predicting graft survival
 and improving the success of kidney transplantations by helping to identify
 compatible donor-recipient pairs.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand cite
key "Analysis of Survival Data"
literal "false"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Cox regression models the relationship between survival time and predictor
 variables using hazard functions, which according to the model can can
 be modeled as product of a baseline hazard function and an exponential
 function of the predictor variables.
\end_layout

\begin_layout Standard
The hazard function describes the probability of an event occurring (such
 as death or failure of a transplanted kidney) at a specific point in time,
 given that the event has not already occurred.
 It is denoted by 
\begin_inset Formula $\lambda\left(t\right)$
\end_inset

 and can be defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\lambda\left(t\right)=\lim_{\Delta t\to0}\frac{P\left(t\leq T<t+\Delta t\mid T\geq t\right)}{\Delta t}\label{eq:hazard function def}
\end{equation}

\end_inset

where T is a random variable representing the time until the event occurs,
 and 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $P\left(t\leq T<t+\Delta t\mid T\geq t\right)$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 represents the probability of the event occurring between time 
\begin_inset Formula $t$
\end_inset

 and time 
\begin_inset Formula $t+\Delta t$
\end_inset

, given that the event has not already occurred by time 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Standard
The Cox proportional hazards model assumes that the hazard function can
 be modeled as follows
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\lambda\left(t\mid\mathbf{X}\right)=\lambda_{0}\left(t\right)\exp\left(\beta_{1}X_{1}+\beta_{2}X_{2}+\ldots+\beta_{k}X_{k}\right)
\end{equation}

\end_inset

where 
\begin_inset Formula $\lambda_{0}\left(t\right)$
\end_inset

 represents the baseline hazard function that is not dependent on the predictor
 variables, k is the number of predictor variables, 
\begin_inset Formula $\mathbf{X}=$
\end_inset


\begin_inset Formula $\left(X_{1},X_{2},\ldots,X_{k}\right)$
\end_inset

 is a vector of predictor variables and 
\begin_inset Formula $\beta_{1},\beta_{2},\ldots,\beta_{k}$
\end_inset

 represent the coefficients associated with each predictor variable.
 The exponential function of the predictor variables represents the proportional
 change in the hazard function associated with a unit change in the correspondin
g predictor variable, holding all other predictor variables constant.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
The hazard ratio (HR) can be calculated for each predictor variable by taking
 the exponential of the coefficient associated with that variable, which
 gives the relative change in the hazard function associated with a one-unit
 increase in that variable, while holding all other variables constant:
\end_layout

\begin_layout Plain Layout
OR
\end_layout

\begin_layout Plain Layout
The hazard ratio (HR) is defined as the ratio of the hazard functions for
 two groups with different values of a predictor variable, while holding
 all other predictor variables constant.
 For example, let :
\end_layout

\begin_layout Plain Layout
HR = λ(t | X1) / λ(t | X2) = exp(β(X1 - X2))
\end_layout

\begin_layout Plain Layout
where X1 and X2 are two different values of the predictor variable, and
 β is the coefficient associated with the predictor variable.
 The hazard ratio represents the proportional change in the hazard function
 associated with a unit change in the predictor variable.
\end_layout

\end_inset

The hazard ratio (
\begin_inset Formula $HR$
\end_inset

) is defined as the ratio of the hazard functions for two groups with different
 values of a predictor variable, while holding all other predictor variables
 constant.
 For example, let 
\begin_inset Formula $\bar{\mathbf{X}}=\left(X_{1},\ldots,X_{i}+1,\ldots,X_{k}\right)$
\end_inset

 then
\begin_inset Formula 
\[
\frac{\lambda\left(t\mid\bar{\mathbf{X}}\right)}{\lambda\left(t\mid\mathbf{X}\right)}=\frac{\lambda_{0}\left(t\right)\exp\left(\beta_{1}X_{1}+\ldots+\beta_{i}\left(X_{i}+1\right)+\ldots+\beta_{k}X_{k}\right)}{\lambda_{0}\left(t\right)\exp\left(\beta_{1}X_{1}+\ldots+\beta_{i}X_{i}+\ldots+\beta_{k}X_{k}\right)}=\frac{\exp\left(\beta_{i}\right)\stackrel[j=1]{k}{\prod}\exp\left(\beta_{j}X_{j}\right)}{\stackrel[j=1]{k}{\prod}\exp\left(\beta_{j}X_{j}\right)}=\exp\left(\beta_{i}\right).
\]

\end_inset

Meaning that the hazard ratio for a given predictor associated with a one-unit
 increase in that variable is given by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
HR_{i}=\exp\left(\beta_{i}\right),\quad i\in\hat{k}.
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
More broadly, the hazard ratio for two groups with varying predictor values
 
\begin_inset Formula $X_{i}^{(1)}$
\end_inset

and 
\begin_inset Formula $X_{i}^{(2)}$
\end_inset

 respectively can be written as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
HR_{i}=\exp\left(\beta_{i}\left(X_{i}^{(1)}-X_{i}^{(2)}\right)\right).
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
As 
\begin_inset Formula $\lambda_{0}\left(t\right)$
\end_inset

 cancels out, the hazard ratio stays constant over time, in another words
 the hazards are 
\emph on
proportional.
\end_layout

\begin_layout Standard
The Cox regression model estimates the coefficients 
\begin_inset Formula $\beta_{i},$
\end_inset


\begin_inset Formula $\forall i\in\hat{k}$
\end_inset

 and the baseline hazard function 
\begin_inset Formula $\lambda_{0}\left(t\right)$
\end_inset

 using 
\emph on
maximum likelihood estimation
\emph default
.
 The model can be used to predict the hazard function for different combinations
 of predictor variables, which can be used to estimate the probability of
 an event occurring at different points in time, given the values of the
 predictor variables.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout

\emph on
maximum likelihood estimation
\emph default
 = Metoda maximální věrohodnosti -> využít něco z PRST??
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The Cox regression model can also be used to estimate the survival function,
 which represents the probability of surviving beyond a given time 
\begin_inset Formula $t$
\end_inset

, given the values of the predictor variables.
 The survival function is defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
S\left(t\mid\mathbf{X}\right)=\exp\left(-\stackrel[0]{t}{\int}\lambda\left(u\mid\mathbf{X}\right)\mathrm{d}u\right)
\end{equation}

\end_inset

where 
\begin_inset Formula $\lambda\left(u\mid\mathbf{X}\right)$
\end_inset

 represents the hazard function at time u, given the values of the predictor
 variables.
\end_layout

\begin_layout Standard
Overall, Cox regression provides a powerful tool for analyzing survival
 data and identifying the factors that influence the likelihood of an event
 occurring over time.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Explain score test - logrank test
\end_layout

\end_inset


\end_layout

\begin_layout Section
Random Survival Forests
\begin_inset CommandInset label
LatexCommand label
name "sec:Random-Survival-Forests"

\end_inset


\end_layout

\begin_layout Standard

\emph on
Random Survival Forests
\emph default
 (RSF) is an ensemble machine learning method used for analyzing right-censored
 survival data.
 RSF combine the concepts of 
\emph on
survival analysis
\emph default
 and 
\emph on
random forests, 
\emph default
which allows it to handle issues commonly associated with conventional survival
 analysis methods, such as restrictive assumptions about the model's parameters
 (e.g.
 proportionality in the Cox Regression model), the inability to handle nonlinear
 relations in the data and issues with missing data, as well as issues associate
d with regular random forest models, such as being restricted to regression
 and classification tasks and the inability to handle right-censored survival
 data 
\begin_inset CommandInset citation
LatexCommand cite
key "Random Survival Forests"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Each tree of the forests ouputs a cummulative hazard function 
\begin_inset Formula $H(t\mid\mathbf{x})$
\end_inset

, which takes an input vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and a set time 
\begin_inset Formula $t$
\end_inset

 and assigns a probability of an event occuring at time 
\begin_inset Formula $t$
\end_inset

 to the individual 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 The forest's overall prediction ,i.e.
 
\emph on
the ensemble commulative hazard,
\emph default
 is then computed as the average of the individual tree predictions.
\end_layout

\begin_layout Standard
Random Survival Forests have been used in a wide range of applications,
 including medicine, engineering, and social sciences, to model survival
 outcomes and identify important prognostic factors.
\end_layout

\begin_layout Subsection
Building a Random Survival Forests model
\end_layout

\begin_layout Standard
The algorithm for building a RSF model works as follows 
\begin_inset CommandInset citation
LatexCommand cite
key "Random Survival Forests"
literal "false"

\end_inset

.
\end_layout

\begin_layout Enumerate
A set number of subsets is taken from the data using the bootstrap sampling
 method, with each sample excluding on average 37% of the original data.
 The excluded data are referred to as 
\emph on
out-of-bag data 
\emph default
(OOB) and they are used for computing the ensemble commulative hazard.
\end_layout

\begin_layout Enumerate
A 
\emph on
binary survival tree 
\emph default
(BST) is built for each of the bootstrap samples.
 The algorithm for growing a BST generally follows the same principles as
 described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Decision-Trees"
plural "false"
caps "false"
noprefix "false"

\end_inset

 , i.e.
 at each node the algorithm iterates over all possible features and values
 and decides on the best split, with the metric used for deciding the split
 being the difference between survival times predicted by the given child
 nodes.
 This is done by maximazing the log-rank statistic described in detail in
 (the split rule is explained in detail in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Log-rank-Test"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Another difference between a BST and a regular decision tree is that each
 node contains not only information about the survival time denoted by 
\begin_inset Formula $T$
\end_inset

, but also binary information about the censoring status denoted by 
\begin_inset Formula $\delta$
\end_inset

.
 The algorithm is restricted by a criterion that each node should contain
 at least 1 unique death, which leads to a point where no new splithts can
 be made meaning the tree has been fully grown.
 Once this point is reached, each terminal node outputs a 
\emph on
cumulative hazard function 
\emph default
(CHF) denoted by 
\begin_inset Formula $\hat{H}(t)$
\end_inset

 which is calculated as follows:
\begin_inset Newline newline
\end_inset

Let 
\begin_inset Formula $A$
\end_inset

 denote the set of all terminal nodes, then each node 
\begin_inset Formula $j\in A$
\end_inset

 contains a set of cases (individuals) 
\begin_inset Formula $\left(T_{1,j},\delta_{1,j}\right),\ldots,\left(T_{n(j),j},\delta_{n(j),j}\right)$
\end_inset

, where 
\begin_inset Formula $n(j)$
\end_inset

 is the number of cases belonging to the node 
\begin_inset Formula $j$
\end_inset

, 
\begin_inset Formula $T_{i,j}$
\end_inset

 is the survival time of the 
\begin_inset Formula $i$
\end_inset

-th case belonging to the node 
\begin_inset Formula $j$
\end_inset

 , 
\begin_inset Formula $\delta_{i,j}\in\left\{ 0,1\right\} $
\end_inset

 is the censoring status of the 
\begin_inset Formula $i$
\end_inset

-th case at time 
\begin_inset Formula $T_{i,j}$
\end_inset

 , with 
\begin_inset Formula $\delta_{i,j}=0$
\end_inset

 if the individual is right-censored (e.g.
 has a functioning graft) and 
\begin_inset Formula $\delta_{i,j}=1$
\end_inset

 if an event has been observered at a given at time 
\begin_inset Formula $T_{i,j}$
\end_inset

 (e.g.
 graft failure).
 Then let 
\begin_inset Formula $t_{1,j}<t_{2,j}<\ldots<t_{N(j),j}$
\end_inset

 denote the time of the different events observed in node 
\begin_inset Formula $j$
\end_inset

, where 
\begin_inset Formula $N(j)=\sum_{i=1}^{n(j)}\delta_{i,j}$
\end_inset

 is the total number of observed events.
 The CHF for the given node is then given by the Nelson-Maier
\begin_inset Note Note
status open

\begin_layout Plain Layout
Nelson-Maier is not a thing according to google search...
 possible referring to either the Kaplan-Meier estimator or the Nelson-Aalen
 estimator.
\end_layout

\end_inset

 estimator
\begin_inset Formula 
\begin{equation}
\hat{H}_{j}(t)=\sum_{t_{k,j}\leq t}\frac{d_{k,j}}{Y_{k,j}}\label{eq:node CHF}
\end{equation}

\end_inset

where 
\begin_inset Formula $d_{k,j}$
\end_inset

 is the number of events observed at time 
\begin_inset Formula $t_{k,j}$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Unclear: should this by 
\begin_inset Quotes sld
\end_inset

at time
\begin_inset Quotes srd
\end_inset

 or 
\begin_inset Quotes sld
\end_inset

by time
\begin_inset Quotes srd
\end_inset

 coz if it's just 
\begin_inset Quotes sld
\end_inset

at time
\begin_inset Quotes srd
\end_inset

 then the sum wouldn't make sense.
\end_layout

\end_inset

 and 
\begin_inset Formula $Y_{k,j}$
\end_inset

 is the number of people at risk at time 
\begin_inset Formula $t_{k,j}$
\end_inset

 i.e.
 the number of individuals that are yet to experience an event.
\begin_inset Newline newline
\end_inset

When a new input vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is introduced to the model, the tree is followed from the top down until
 a terminal node 
\begin_inset Formula $j\in A$
\end_inset

 is reached.
 The CHF of the whole tree is then given by
\begin_inset Formula 
\begin{equation}
H(t\mid\mathbf{x})=\hat{H}_{j}(t).\label{eq:tree CHF}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate
Next, two ensemble commulative hazards functions are calculated.
 The first, called the 
\emph on
bootstrap ensemble CHF
\emph default
 (denoted by 
\begin_inset Formula $H_{e}^{*}),$
\end_inset

simply averages over all grown survival trees.
 The second, called the 
\emph on
OOB ensemble CHF
\emph default
 (denoted by 
\begin_inset Formula $H_{e}^{**}$
\end_inset

), averages over trees where a given input was not used in the training
 of the given tree i.e.
 is a case of the OOB data.
 Suppose an example 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

 is taken from the training dataset, let 
\begin_inset Formula $B$
\end_inset

 denote the total number of bootstrap samples, and let 
\begin_inset Formula $H_{b}^{*}(t|\mathbf{x})$
\end_inset

 denote the CHF predicted by the tree grown from the 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
bootstrap sample 
\begin_inset Formula $b\in\hat{B}$
\end_inset

 and let
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula 
\[
I_{i,b}=\begin{cases}
1 & \textrm{if \ensuremath{\mathbf{x}_{i}} belongs to the OOB subset of the dataset for bootstrap sample \ensuremath{b}}\\
0 & \textrm{else, i.e. \ensuremath{\mathbf{x}_{i}} was used in the training of a tree grown from the bootstrap sample \ensuremath{b} }
\end{cases}.
\]

\end_inset

The bootstrap ensemble CHF is then defined as
\begin_inset Formula 
\begin{equation}
H_{e}^{*}(t\mid\mathbf{x}_{i})=\frac{1}{B}\sum_{b=1}^{B}H_{b}(t\mid\mathbf{x}_{i})
\end{equation}

\end_inset

where 
\begin_inset Formula $H_{b}(t\mid\mathbf{x}_{i})$
\end_inset

 is the CHF of a tree grown from the bootstrap sample 
\begin_inset Formula $b$
\end_inset

, defined in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:tree CHF"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\begin_inset Newline newline
\end_inset

In contrast, the OOB ensemble CHF is defined as 
\begin_inset Formula 
\begin{equation}
H_{e}^{**}(t\mid\mathbf{x}_{i})=\frac{\sum_{b=1}^{B}I_{i,b}H_{b}(t\mid\mathbf{x}_{i})}{\sum_{b=1}^{B}I_{i,b}}.\label{eq:OOB ensemble CHF}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Split rules
\begin_inset CommandInset label
LatexCommand label
name "subsec:Split-rules"

\end_inset


\end_layout

\begin_layout Standard
TODO: Explain the log-rank statistic.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Copied from [5]:
\end_layout

\begin_layout Plain Layout
Computations were implemented using randomSurvivalForest software under
 its default settings [Ishwaran and Kogalur (2007, 2008)].
 In each instance 1000 trees were grown.
 Each of the four splitting rules available in the software package were
 used.
 These were as follows [for more details see Ishwaran and Kogalur (2008)]:
\end_layout

\begin_layout Enumerate
A log-rank splitting rule (logrank) that splits nodes by maximization of
 the log- rank test statistic [Segal (1988), LeBlanc and Crowley (1993)].
\end_layout

\begin_layout Enumerate
A conservation-of-events splitting rule (conserve) that splits nodes by
 finding daughters closest to the conservation-of-events principle.
\end_layout

\begin_layout Enumerate
3.
 A log-rank score rule (logrankscore) that splits nodes using a standardized
 log- rank statistic [Hothorn and Lausen (2003)].
\end_layout

\begin_layout Enumerate
4.
 A random log-rank splitting rule (logrankrandom).
 A random split is selected for each of the p candidate variables in a node,
 and the variable with maximum log-rank statistic (at its random split point)
 is used to split the node.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
https://www.randomforestsrc.org/articles/survival.html
\end_layout

\begin_layout Plain Layout
The true event time being subject to censoring must be dealt with when growing
 a RSF tree.
 In particular, the splitting rule for growing the tree must specifically
 account for censoring.
 Thus, the goal is to split the tree node into left and right daughters
 with dissimilar event history (survival) behavior.
\end_layout

\begin_layout Plain Layout
Log-rank splitting The default splitting rule used by the package is the
 log-rank test statistic and is specified by splitrule="logrank".
 The log-rank test has traditionally been used for two-sample testing with
 survival data, but it can be used for survival splitting as a means for
 maximizing between-node survival differences [2–6].
\end_layout

\begin_layout Plain Layout
To explain log-rank splitting, consider a specific tree node to be split.
 Without loss of generality let us assume this is the root node (top of
 the tree).
 For simplicity assume the data is not bootstrapped, thus the root node
 data is (𝑇1,𝐗1,𝛿1),…,(𝑇𝑛,𝐗𝑛,𝛿𝑛) .
 Let 𝑋 denote a specific variable (i.e., one of the coordinates of the feature
 vector).
 A proposed split using 𝑋 is of the form 𝑋≤𝑐 and 𝑋>𝑐 (for simplicity we
 assume 𝑋 is nominal) and splits the node into left and right daughters,
 𝐿={𝑋𝑖≤𝑐} and 𝑅={𝑋𝑖>𝑐} , respectively.
 Let 𝑡1<𝑡2<⋯<𝑡𝑚 be the distinct death times and let 𝑑𝑗,𝐿,𝑑𝑗,𝑅 and 𝑌𝑗,𝐿,𝑌𝑗,𝑅
 equal the number of deaths and individuals at risk at time 𝑡𝑗 in daughter
 nodes 𝐿,𝑅 .
 At risk means the number of individuals in a daughter who are alive at
 time 𝑡𝑗 , or who have an event (death) at time 𝑡𝑗 : 𝑌𝑗,𝐿=#{𝑇𝑖≥𝑡𝑗,𝑋𝑖≤𝑐},𝑌𝑗,𝑅=#{𝑇
𝑖≥𝑡𝑗,𝑋𝑖>𝑐}.
 Define 𝑌𝑗=𝑌𝑗,𝐿+𝑌𝑗,𝑅,𝑑𝑗=𝑑𝑗,𝐿+𝑑𝑗,𝑅.
 The log-rank split-statistic value for the split is 𝐿(𝑋,𝑐)=∑𝑗=1𝑚(𝑑𝑗,𝐿−𝑌𝑗,𝐿𝑑𝑗𝑌𝑗)
∑𝑗=1𝑚𝑌𝑗,𝐿𝑌𝑗(1−𝑌𝑗,𝐿𝑌𝑗)(𝑌𝑗−𝑑𝑗𝑌𝑗−1)𝑑𝑗‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾⎷.
 The value |𝐿(𝑋,𝑐)| is a measure of node separation.
 The larger the value, the greater the survival difference between 𝐿 and
 𝑅 , and the better the split is.
 The best split is determined by finding the feature 𝑋∗ and split-value
 𝑐∗ such that |𝐿(𝑋∗,𝑐∗)|≥|𝐿(𝑋,𝑐)| for all 𝑋 and 𝑐 .
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
The math behind RSF can be broken down into the following steps:
\end_layout

\begin_layout Enumerate
At each node in the decision tree, a subset of p variables is randomly selected
 from the total set of predictors.
 The best variable is selected as the one that maximizes the difference
 in survival times between the two subsets of data created by the split.
 The log-rank test is commonly used as the criterion for measuring the differenc
e in survival times between the two groups.
 The log-rank test statistic is defined as:
\begin_inset Graphics
	filename pasted1.png

\end_inset

where I(s) and I(t) are the indices of the individuals in the two subsets
 created by the split, Y_i is the survival time of individual i, and delta_i
 is an indicator variable that takes the value 1 if individual i experiences
 an event (i.e., death) and 0 otherwise.
\end_layout

\begin_layout Enumerate
Once the best variable has been selected, the data is split into two subsets
 based on a threshold value.
 The threshold value is chosen such that the difference in survival times
 between the two groups is maximized.
\end_layout

\begin_layout Enumerate
The process is repeated recursively for each subset of data until a stopping
 criterion is met.
 This typically occurs when a node contains a minimum number of individuals
 or when the maximum tree depth is reached.
\end_layout

\begin_layout Enumerate
4: Once all the decision trees have been constructed, they are aggregated
 to create the random survival forest.
 The forest's overall prediction for the survival time of an individual
 is computed as the average of the individual tree predictions.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Let's consider a dataset with 
\begin_inset Formula $n$
\end_inset

 individuals, where each individual is described by a 
\begin_inset Formula $k$
\end_inset

-dimensional vector of predictor variables 
\begin_inset Formula $\mathbf{x}=\left(x_{1},\ldots,x_{k}\right)$
\end_inset

, and has an associated survival time 
\begin_inset Formula $t$
\end_inset

 and censoring status 
\begin_inset Formula $c$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
What is censoring status?
\end_layout

\end_inset

 The censoring status indicates whether the survival time is observed or
 right-censored (i.e., the event has not occurred by the end of the study).
 The survival function 
\begin_inset Formula $S\left(t\mid x\right)$
\end_inset

 is defined as the probability of surviving beyond time 
\begin_inset Formula $t$
\end_inset

, given the predictor variables 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Plain Layout
The RSF algorithm can be summarized as follows:
\end_layout

\begin_layout Plain Layout
1.Generate B bootstrap samples of the data: For each bootstrap sample, grow
 a decision tree as follows:
\end_layout

\begin_layout Itemize
a.
 Select a random subset of predictor variables of size m (where m << p).
\end_layout

\begin_layout Itemize
b.
 For each node of the tree, select the best split point based on the log-rank
 test statistic, which measures the difference in survival times between
 the two subsets created by the split.
\end_layout

\begin_layout Itemize
c.
 Repeat steps a and b until the tree is fully grown.
\end_layout

\begin_layout Plain Layout
2.
 Aggregate the B trees to make predictions for a new individual:
\end_layout

\begin_layout Itemize
a.
 For each tree, find the terminal node to which the new individual belongs.
\end_layout

\begin_layout Itemize
b.
 Compute the survival probability for the new individual as the Kaplan-Meier
 estimate of the survival function for the individuals in the terminal node.
\end_layout

\begin_layout Itemize
Compute the final prediction for the new individual as the average of the
 survival probabilities computed in step 3.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Calculating Prediction Error
\end_layout

\begin_layout Standard
The prediction error of random survival forest models is calculated using
 the 
\emph on
Harell's concordance index 
\emph default
(also known as 
\emph on
C-index
\emph default
) 
\begin_inset CommandInset citation
LatexCommand cite
key "Random Survival Forests"
literal "false"

\end_inset

, which is a metric commonly used in medical research as well as machine
 learning to evaluate the performance of predictive models.
 The C-index is equal to the are under the 
\emph on
ROC curve
\emph default
 (short for Receiver Operating Characteristics curve), which is a graphical
 plot that illustrates the performance of a binary classification model
 at different classification thresholds 
\begin_inset CommandInset citation
LatexCommand cite
after "Classification - ROC Curve and AUC"
key "ML Crash Course"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
https://developers.google.com/machine-learning/crash-course/classification/roc-and
-auc#:~:text=An%20ROC%20curve%20(receiver%20operating,False%20Positive%20Rate
\end_layout

\end_inset

Specifically, it plots the 
\emph on
true positive rate 
\emph default
(TPR)
\emph on
 
\emph default
defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
TPR=\frac{TP}{TP+FN}
\end{equation}

\end_inset

where 
\begin_inset Formula $TP$
\end_inset

 is the number of 
\emph on
true positives
\emph default
 (i.e.
 positive cases correctly classified
\emph on
 
\emph default
by the model
\emph on
) 
\emph default
and 
\begin_inset Formula $FN$
\end_inset

 is the number 
\emph on
false negatives
\emph default
 (i.e.
 positive cases incorrectly classified as negative), against the 
\emph on
false positive rate 
\emph default
(FPR) defined as
\begin_inset Formula 
\begin{equation}
FPR=\frac{FP}{FP+TN}
\end{equation}

\end_inset

where 
\begin_inset Formula $FP$
\end_inset

 is the number of 
\emph on
false positives (i.e.
 
\emph default
negative cases incorrectly classified as positive) and 
\begin_inset Formula $TN$
\end_inset

 is the number of 
\emph on
true negatives 
\emph default
(i.e.
 negative cases correctly classified as negative).
\end_layout

\begin_layout Standard
Lowering the classification threshold results in more cases being classified
 as positive, thus generally increasing both the true positive and the false
 negative rate.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/1024px-Roc_
curve.svg.png
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The area under the ROC curve can be interpreted as the probability that
 a given model will rank a random positive example more highly (e.g.
 it will assign a more favorable hazard function) than a random negative
 example.
 The C-index can attain values from 0 to 1 with 1 meaning a perfect model
 whose predictions are always correct and 0 meaning it's always wrong.
 A random classifier that assigns the predicted value with a uniform distributio
n, will have a C-index of 0.5.
 Generally, models with a C-index above 0.7 are considered to be good predictors.
\end_layout

\begin_layout Standard
In the case of RSF, it isn't necessary to know the function of the ROC curve
 explicitly, rather the C-index is calculated as follows 
\begin_inset CommandInset citation
LatexCommand cite
key "Random Survival Forests"
literal "false"

\end_inset

:
\end_layout

\begin_layout Enumerate
Find all possible pairs of cases across the dataset while excluding the
 following cases: 
\end_layout

\begin_deeper
\begin_layout Itemize
pairs 
\begin_inset Formula $\left((T_{i},\delta_{i}),(T_{j},\delta_{j})\right)$
\end_inset

 where the case with the shorter survival time is right-censored
\begin_inset Newline newline
\end_inset

 i.e.
 
\begin_inset Formula $T_{i}<T_{j}\:\land\:\delta_{i}=0$
\end_inset


\end_layout

\begin_layout Itemize
pairs 
\begin_inset Formula $\left((T_{i},\delta_{i}),(T_{j},\delta_{j})\right)$
\end_inset

 with equal survival times where both cases are also right-censored
\begin_inset Newline newline
\end_inset

i.e.
 
\begin_inset Formula $T_{i}=T_{j}\:\land\:\delta_{i}=\delta_{j}=0$
\end_inset


\end_layout

\begin_layout Standard
The cases that are left are called 
\emph on
permissible
\emph default
 and we denote their set by 
\begin_inset Formula $P.$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
For each permissible pair 
\begin_inset Formula $\left((T_{i},\delta_{i}),(T_{j},\delta_{j})\right)$
\end_inset

 a value for 
\begin_inset Formula $c_{i,j}$
\end_inset

 is assigned as follows:
\end_layout

\begin_deeper
\begin_layout Enumerate
if 
\begin_inset Formula $T_{i}\neq T_{j}$
\end_inset

 then
\begin_inset Formula 
\[
c_{i,j}=\begin{cases}
1 & \textrm{if \ensuremath{T_{i}<T_{j}\Longrightarrow i} has worse predicted outcome than \ensuremath{j} (\ref{eq:worse outcome def})}\\
0.5 & \textrm{if }\ensuremath{T_{i}<T_{j}\Longrightarrow}\textrm{predicted outcomes of \ensuremath{i} and \ensuremath{j} are tied}\\
0 & \textrm{else}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Enumerate
if 
\begin_inset Formula $T_{i}=T_{j}$
\end_inset


\begin_inset Formula $\:\land\:\delta_{i}=\delta_{j}=1$
\end_inset

 
\begin_inset Formula 
\[
c_{i,j}=\begin{cases}
1 & \textrm{if predicted outcomes of \ensuremath{i} and \ensuremath{j} are tied}\\
0.5 & \textrm{else}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Enumerate
if 
\begin_inset Formula $T_{i}=T_{j}$
\end_inset


\begin_inset Formula $\:\land\:\delta_{i}\neq\delta_{j}$
\end_inset

 
\begin_inset Formula 
\[
c_{i,j}=\begin{cases}
1 & \textrm{if }\delta_{i}=1\Longrightarrow\textrm{\ensuremath{i} has worse predicted outcome than \ensuremath{j} }\\
0.5 & \textrm{else}
\end{cases}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
The C-index is then given by
\begin_inset Formula 
\begin{equation}
C=\frac{1}{\left|P\right|}\sum_{i,j\in P,i\neq j}c_{i,j}.
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Whether a given case has a better or a worse predicted outcome is decided
 as follows.
 Let 
\begin_inset Formula $t_{1},\ldots t_{n}$
\end_inset

 be a set of pre-chosen time points, then case 
\begin_inset Formula $i$
\end_inset

 has a worse predicted outcome than case 
\begin_inset Formula $j$
\end_inset

 if
\begin_inset Formula 
\begin{equation}
\sum_{k=1}^{n}H_{e}^{**}(t_{k}\mid\mathbf{x}_{i})<\sum_{k=1}^{n}H_{e}^{**}(t_{k}\mid\mathbf{x}_{j})\label{eq:worse outcome def}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathbf{x}_{l}$
\end_inset

 is a feature vector of the case 
\begin_inset Formula $l,$
\end_inset

and 
\begin_inset Formula $H_{e}^{**}$
\end_inset

 is the OOB ensemble CHF defined in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:OOB ensemble CHF"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard
The prediction 
\emph on
error
\emph default
 (
\begin_inset Formula $PE$
\end_inset

) is then simply given by 
\begin_inset Formula $PE=1-C$
\end_inset

.
 
\end_layout

\begin_layout Section
Other Methods
\end_layout

\begin_layout Standard
TODO: Mention some of the other methods referrenced in the related papers
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0209068#sec008
\end_layout

\begin_layout Plain Layout
Harrell’s concordance index - used to measure performance in this and some
 other papers regarding post transplant survival, - Harrell’s concordance
 index (C-index) [8], which is the percentage of patient pairs correctly
 “ranked” by the model based on their post-transplant survival duration
 in a given timeframe combines predictions from random survival forests
 with a Cox proportional hazards model The Cox proportional hazards model
 is the most widely used model for kidney transplant survival estimation
 the data was provided by UNOS which should be the same dataset we are using
 transplantations performed before and after 2002 have a big difference
 in survival curves so only the transplantations performed from 2002-2012
 were taken into account in this paper (it was published in 2017 and they
 required a 5 year or longer time window) 487 variables, removed the ones
 that were not present in more than 95% of entries unless they were labelled
 as important by previous papers -> resulting dataset had 73 variables for
 variable selection, Breiman-Cutler permutation importance measure for random
 survival forests was used to rank the variables by importance recipient
 age came out as most important so the data was split into age based cohorts
 the split age was calculated to be 50 these are the most important factors
 for each cohort:
\end_layout

\begin_layout Plain Layout
methodology: 
\end_layout

\begin_layout Plain Layout
Identify important predictive variables by performing variable selection
 techniques such as Lasso or permutation importance.
 Test the performance of multiple predictive models on the data using the
 variables identified in step 1.
 Use cross-validation and metrics such as the concordance index to evaluate
 the performance.
 Determine the best binary split in the data using methods such as decision
 trees.
 Repeat steps 1–3 for both subsets of the data a specified number of times.
 The final model consists of combining the predictions from the models that
 perform best on the different subsets of the data.
 
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
https://www.sciencedirect.com/science/article/pii/S1386505619302977?casa_token=IzO
XSEZAHvYAAAAA:qk8ffTkNWAkLvE7vXCExvvOkgy-j0s91ktcFHZZF_Z1AKV9Xf9Bjoz6t7IsrcSDW37
7-Dpbh0vE#bibl0005 is a review of previously used machine learning models
 predicting kidney transplant survival most frequently used methods were
 artificial neural networks, decision trees and Bayesian belief networks
 most had binary outputs (fail or not fail), only one did survival time
 currently used predictive models are bassed on regression methods - logistic
 and cox regression they divided studies into the ones that used only pre-transp
lant data, post-transplant data, and all-data ( I will be interested only
 in pre-transplant data) a good introduction: “Increasing prevalence of
 Chronic Kidney Disease (CKD) and end stage of kidney disease over recent
 years has resulted in increased demand for kidney replacement therapy (KRT)
 [1,2].
 Among the available KRT modalities, kidney transplantation has demonstrated
 superior quality of life and survival rates [3].
 However, health systems around the world have not been able to meet the
 growing demand for kidney grafts” , “Machine learning (ML) is a suite of
 methods whose theoretical construct may lead to improved predictive performance
 over conventional statistical modelling [13](Supplementary File 1) ML is
 an efficient way of analysing large quantities of data and identifying
 hidden associations in complex data sets [14,15].
 ML has evolved dramatically over recent decades and is already commonly
 used in medical diagnostics [16,17].
 Its use in building predictive models to diagnose different disease conditions
 continues to expand” “Prediction models developed using Cox regression
 were compared to ML models developed using artificial neural network by
 Akl et al.
 (2008) [22] and Lin et al.
 (2008) [24].
 According to Akl et al.
 (2008) [22], the prediction accuracy (artificial neural network 95% vs
 Cox 90%) and AUC (artificial neural network 0.88 vs Cox 0.72) of artificial
 neural network were 5% and 0.16 higher compared to the Cox regression model.”
 The developed ML models although often more accurate in predicting compatibilit
y showed bigger inconsistencies than traditional predictive methods like
 the logistic and cox regression “It was interesting to note that models
 developed using a small number of cases reported similar prediction accuracy
 compared with models developed using larger numbers of cases.
 For example, the prediction model developed by Lofaro et al.
 (2010) [32] using 80 records performed similar to the model developed by
 Krikov et al.
 (2007) [29], which was built on a national database of 92,844 patient records.”
 Using machine learning models could help circumvent human bias though it’s
 debatable whether that’s always desirable “The current consensus is that
 there is no one method that fits all data sets, with the complexity of
 the data pivotal [49].
 To get around this uncertainty investigators use multiple machine learning
 methods on single data sets and the best is chosen based on validation
 parameters.“ “Validation parameters have not yet been standardised in the
 literature.
 The studies included in the review have used different validation parameters
 such as sensitivity, area under the curve and accuracy.
 Standardisation of validation parameters will facilitate comparisons between
 different models” “Conventional models such as Cox models and logistic
 regression assume that the predictors are independent of each other “ thus
 they not optimal for handling non-linear relationships between “predictors”
 (find definition) and outcomes “Most of the machine learning based predictive
 models predicted graft failure with high sensitivity and specificity” ---
 what does sensitivity and specificity mean
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0252069 The
 aim of this one is predicting the waiting time on a kidney transplant waiting
 list Cox regression models was used, sensitivity analysis was performed
 using different Cox models “The major characteristics associated with changes
 in the likelihood of transplantation were age, subregion, cPRA, and frequency
 of HLA-DR, -B and -A.” “Although approximately 5,000 deceased donor kidney
 transplants take place yearly in the country, the supply of organs does
 not meet demand, and the gap is growing [2, 3].
 As a result, recurrent tests and procedures are necessary every 2–3 years
 to maintain patients on active transplant list [4].
 Since this poses a significant economic burden to the healthcare system,
 predicting a patient’s waiting time can help planning for pre-transplant
 evaluation, and thus promote a more efficient use of resources “ “estimating
 waiting time on the transplant list can help identifying the underprivileged,
 and thus impact allocation score, bringing more equity to transplantation
 programs” a good sentence: “this study aimed at identifying the relevant
 predictors and combine them into a predictor model to estimate time on
 a kidney transplant waiting list using machine learning.” “Allocation was
 performed as established by the National Transplantation System of the
 Brazilian Ministry of Health [12, 13].
 For deceased donor transplants, allocation criteria are based on HLA matching
 (highest number of points for HLA DR, followed by HLA B and HLA A), recipient’s
 age (<18 years), date of registration on the waiting list, and panel reactive
 antibody (PRA).
 A point score system based on blood group and HLA match is used as follows:
 DR: 0 MM = 10 points; 1 MM = 5 points; 2 MM = 0 point; B: 0 MM = 4 points;
 1 MM = 2 points; 2 MM = 0 point; A: 0 MM = 1 point; 1 MM = 0.5 point; 2
 MM = 0 point.
 “The following variables were evaluated as predictors: age, sex, race,
 comorbidities, time on dialysis, blood group, calculated panel class I
 (cPRA), HLA-A, HLA-B, HLA-DR, number of blood transfusions, pregnancies,
 previous kidney transplants, and pre-transplant serology for Hepatitis
 B and C.” “During the study period, 28.4% of the patients were transplanted
 with a deceased donor (Fig 1).
 The median waiting time for transplantation was 26.3 months.”
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
https://journals.lww.com/asaiojournal/Fulltext/2007/09000/Predicting_Kidney_Transp
lant_Survival_Using.11.aspx “The goal of this project was to develop the models
 predicting probability of kidney allograft survival at 1, 3, 5, 7, and
 10 years” ( allograft = The transplant of an organ, tissue, or cells from
 one individual to another individual, aloštěp) Tree-based forests were
 used “We generated five separate logistic regression models predicting
 the graft survival as a binary variable at 1, 3, 5, 7, and 10 years of
 the follow-up.
 We used a conservative approach to variable selection.
 Only variables that had significant association (p < 0.05) with the outcome
 in all of 5 models were included in the final tree-based analysis.
 In other words, variables that were not significant in at least one model
 were excluded.” “Using the set of variables selected by these methods, we
 tested the tree-based model for convergence and demonstrated poor performance,
 which were thought to be due to potential collinearity in the data.
 To make the model more practical and parsimonious, we evaluated the performance
 of the model with the shorter list of variables, excluding the variables
 that were considered nonessential.
 Heartbeating donor variable was found to have significant missing information,
 whereas nonmissing data were collinear with donor type (living versus deceased).
 Variables describing cardiovascular disease history were collinear with
 the variable describing peripheral vascular disease history and therefore
 the latter was removed.
 The variable describing the use of antihypertensive medications by the
 donor was largely homogenous and was also removed.
 RRT modality immediately before transplant was not used, and instead predominan
t RRT modality during ESRD course was included in the model.
 We also excluded the variable describing dialysis network because the model
 did not converge in its presence.
 Based on R2 statistics, the model based on the shorter list of variables
 (below) performed not worse than the one less parsimonious based on the
 longer list of predictors.” “The final list included the following recipient
 variables: recipient race, gender, age, height, weight, recipient having
 a transplant before the current one (yes/no), total number of transplants
 (including the current one), the time recipient has been on the list before
 transplant, predominant renal replacement therapy modality, percent time
 on peritoneal dialysis before transplant, number of renal replacement therapy
 modalities used before transplant, specific combination of renal replacement
 therapy modalities, recipient comorbidity score, history of cardiovascular
 disease, history of unstable angina, history of diabetes, history of hypertensi
on, presence of hepatitis B core antibodies, presence of hepatitis C antibodies,
 peak and most recent level of panel reactive antibodies, and primary source
 of pay for medical services.
 In addition, the following donor variables were used in the final model:
 donor race, gender, age, height, weight, donor type (living or deceased).”
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Model training
\end_layout

\begin_layout Section
Data Acquisition
\end_layout

\begin_layout Standard
The data used for training was obtained from the United Network for Organ
 Sharing (UNOS) which is a nonprofit organization that manages the organ
 transplant system in the United States.
 The organization oversees transplantation procedures (matching donors,
 ensuring fair graft allocation) for multiple organs such as kidneys, heart,
 lungs, liver, pancreas and intestines.
 As a result, UNOS maintains a large database of organ transplantations
 carried between 1984 and today.
 Specifically, there are more than a million entries regarding kidney transplant
ations, an amount sufficient for training a machine learning model.
\end_layout

\begin_layout Standard
The data can be obtained free of charge via 
\emph on
https://unos.org
\emph default
 (a VPN may be needed for access from outside the US) upon filling out the
 request form on the website and signing relevant documents regarding the
 use of the data that will be sent via a provided email.
 The data was obtained in the form of a tab-delimited file that was transformed
 into a MongoDB database.
 The database tables were then converted into a JSON format using the Database
 tools plugin in IntelliJ IDEA Ultimate.
 
\end_layout

\begin_layout Standard
Alongside the data from the UNOS database, another dataset was provided
 by the Czech Institute for Clinical and Experimental Medicine (IKEM), however
 the dataset was limited in size and thus was not suitable for training
 a machine learning model.
\end_layout

\begin_layout Section
Software Architecture
\end_layout

\begin_layout Standard
A number of software tools and libraries were used for building the survival
 analysis machine learning model, with the backbone of the application being
 the Python based 
\emph on
Scikit-survival 
\emph default
package, which is itself built upon the 
\emph on
Scikit-learn
\emph default
 package.
 Additionally, the 
\emph on
pandas
\emph default
 and 
\emph on
numpy
\emph default
 libraries were used for both preprocessing of the data and evaluating the
 models performance and finally, the 
\emph on
matplotlib
\emph default
 library was used to provide graphical output in order visually validate
 models performance.
\end_layout

\begin_layout Subsection
Python
\end_layout

\begin_layout Standard

\emph on
Python
\emph default
 is an interpreted, general-purpose, high-level, object-oriented programming
 language that consistently ranks among the most used programming languages
 in the world 
\begin_inset CommandInset citation
LatexCommand cite
key "Octoverse"
literal "false"

\end_inset

.
 The reasons for its popularity include relatively easy human readability,
 built-in data structures such as lists and dictionaries, wide range of
 libraries that provide additional functionality, automatic memory management
 and the fact that Python is cross platform, meaning that code written on
 one machine can easily be run on different machines, regardless of their
 operating system.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Python docs,Python wiki"
literal "false"

\end_inset


\end_layout

\begin_layout Subsection
Jupyter Notebook
\end_layout

\begin_layout Standard

\emph on
Jupyter Notebook 
\emph default
is a notebook authoring web application that provides tools for interactive
 computing with 
\emph on
computational notebooks 
\emph default
(also knows as 
\emph on
notebook interface
\emph default
).
 Computational notebooks are documents that enable users to integrate executable
 computer code, plain text, visualization and other interactive tools in
 a structured manner.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Jupyter Notebook"
literal "false"

\end_inset

 One of the fundamental advantages of computational notebooks is the ability
 to partition code into smaller self-contained units with each unit having
 the result of their execution attached to them.
 This facilitates easier analysis of results and makes information extraction
 more efficient.
 This proves to be beneficial especially in the fields of data analysis
 and machine learning where extracting information from data is of utmost
 importance.
 Jupyter Notebooks are primarily used for executing Python code.
 Nonetheless, as indicated by its name, Jupyter Notebooks also provide support
 for the Julia and R languages.
\end_layout

\begin_layout Subsection
scikit-learn and scikit-survival
\end_layout

\begin_layout Subsubsection*
scikit-learn
\end_layout

\begin_layout Standard

\emph on
scikit-learn
\emph default
 is an open-source machine learning Python module.
 It is written in Python and C and it is the most frequently used library
 for machine learning 
\begin_inset CommandInset citation
LatexCommand cite
key "100-page ML"
literal "false"

\end_inset

.
 scikit-learn provides tools for every part of the process of building a
 machine learning model, namely data preprocessing, model selection, fitting,
 and evaluation, among others 
\begin_inset CommandInset citation
LatexCommand cite
key "scikit-learn"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection*
scikit-survival
\end_layout

\begin_layout Standard

\emph on
scikit-survival 
\emph default
is a Python module built on top of scikit-learn.
 It utilizes the functionalities of scikit-learn for machine learning, while
 allowing the user to implement survival analysis methods in their models
 
\begin_inset CommandInset citation
LatexCommand cite
key "scikit-survival"
literal "false"

\end_inset

.
 Included in the library are many of the standard data analysis methods
 including the Kaplan-Meier estimator 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Kaplan-Meier-Estimator"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and Cox Regression 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Cox-Regression"
plural "false"
caps "false"
noprefix "false"

\end_inset

 as well as methods combining survival analysis with machine learning methods
 such as the Random Survival Forests 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Random-Survival-Forests"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\emph on
Survival Support Vector Machine 
\emph default
and 
\emph on
Gradient Boosted Models .
\end_layout

\begin_layout Subsection
Other libraries 
\end_layout

\begin_layout Standard
TODO: Write about the other packages
\end_layout

\begin_layout Subsection
Cluster Computing
\end_layout

\begin_layout Standard
Due to the large size of the tables extracted from the UNOS dataset, reaching
 up to 40GB in the JSON format, conducting any meaningful operations locally
 became infeasible since in order to do any sort of processing, the data
 needed to be first loaded into main memory, which, in my case, had a capacity
 of only 12GB.
 As a result, all computing was conducted remotely on the HELIOS high-performanc
e computing cluster hosted at the Department of Mathematics, Faculty of
 Nuclear Sciences and Physical Engineering, Czech Technical University in
 Prague.
 The cluster can provide up to 384GB of main memory on a single node, which
 was more than enough for the needs of this project.
 The connection to the cluster was established using SSH in the Linux shell.
 To enhance practicality, I configured node forwarding from the remote node
 to localhost, which allowed me to interact with Jupyter notebooks directly
 via a web browser on my machine.
 More information about the cluster such as about the hardware specs can
 be found at 
\begin_inset CommandInset citation
LatexCommand cite
key "HELIOS"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Traing the model
\end_layout

\begin_layout Standard
Here I will possible include some code but mostly graphs.
\end_layout

\begin_layout Section
Optimization
\end_layout

\begin_layout Standard
Trying different things to get better results.
\end_layout

\begin_layout Chapter
Model evaluation
\end_layout

\begin_layout Section
General evaluation
\end_layout

\begin_layout Standard
Discussion about the results.
\end_layout

\begin_layout Section
Performance on different population samples
\end_layout

\begin_layout Standard
Try to locally differentate ethnic groups within the UNOS database, possibly
 try to fit the model onto the IKEM dataset.
\end_layout

\begin_layout Section
Comparison to conventional models
\end_layout

\begin_layout Standard
Is this even an improvement?
\end_layout

\begin_layout Chapter*
Conclusion
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{plain}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{Conclusion}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Text of the conclusion\SpecialChar ldots

\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Possible sources:
\end_layout

\begin_layout Plain Layout
Cox Regression:
\end_layout

\begin_layout Itemize
Cox, D.
 R.
 (1972).
 Regression models and life tables (with discussion).
 Journal of the Royal Statistical Society, Series B, 34, 187-220.
 
\end_layout

\begin_layout Itemize
Kleinbaum, D.
 G., & Klein, M.
 (2012).
 Survival analysis: A self-learning text (3rd ed.).
 (Mentioned multiple times)
\end_layout

\begin_layout Itemize
Springer.
 Allison, P.
 D.
 (2010).
 Survival analysis using SAS: A practical guide (2nd ed.).
 SAS Institute.
 
\end_layout

\begin_layout Itemize
Collett, D.
 (2003).
 Modelling survival data in medical research (2nd ed.).
 Chapman & Hall/CRC.
 
\end_layout

\begin_layout Itemize
Singer, J.
 D., & Willett, J.
 B.
 (1993).
 It's about time: Using discrete-time survival analysis to study duration
 and the timing of events.
 Journal of Educational Statistics, 18(2), 155-195.
\end_layout

\begin_layout Plain Layout
Survival analysis:
\end_layout

\begin_layout Itemize
https://www2.karlin.mff.cuni.cz/~pesta/NMFM404/survival.html#References
\end_layout

\begin_layout Itemize
https://www2.karlin.mff.cuni.cz/~pesta/NMFM404/ph.html
\end_layout

\begin_layout Itemize
https://www2.karlin.mff.cuni.cz/~pesta/NMFM404-2021.html
\end_layout

\begin_layout Plain Layout
Other:
\end_layout

\begin_layout Itemize
Therneau, T., & Grambsch, P.
 (2000).
 Modeling survival data: extending the Cox model.
 Springer Science & Business Media.
\end_layout

\begin_layout Itemize
Breiman, L.
 (2001).
 Random forests.
 Machine learning, 45(1), 5-32
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Nice sentences:
\end_layout

\begin_layout Itemize
The process is repeated in a recursive fashion for each subsequent node.
\end_layout

\begin_layout Itemize
By max- imizing survival difference, the tree pushes dissimilar cases apart.
 Eventually, as the number of nodes increase, and dissimilar cases become
 separated, each node in the tree becomes homogeneous and is populated by
 cases with similar survival.
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Chronic kidney allograft loss"
literal "false"

\end_inset

B.
 J.
 Nankivell, D.
 R.
 Kuypers.
 
\emph on
Diagnosis and prevention of chronic kidney allograft loss, 
\emph default
The Lancet, Vol.
 378, Issue 9800, 2011, 1428-1437
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Systematic review"
literal "false"

\end_inset

S.
 Senanayake, N.
 White, N.
 Graves, H.
 Healy, K.
 Baboolal, S.
 Kularatna.
 
\emph on
Machine learning in predicting graft failure following kidney transplantation:
 A systematic review of published predictive models
\emph default
, International Journal of Medical Informatics, Volume 130, 2019, 103957
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Dialysis"
literal "false"

\end_inset

D.D.
 Aufhauser Jr., et al.
 
\emph on
Impact of prolonged dialysis prior to renal transplantation.

\emph default
 Clin Transplant, 2018 Jun, 32(6):e13260
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Using ML methods to predict kidney transplant survival"
literal "false"

\end_inset

E.
 Mark, D.
 Goldsman, B.
 Gurbaxani, P.
 Keskinocak, J.
 Sokol.
 
\emph on
Using machine learning and an ensemble of methods to predict kidney transplant
 survival
\emph default
.
 PLoS ONE 14(1), 2019, e0209068.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "100-page ML"
literal "true"

\end_inset

A.
 Burkov: 
\emph on
The Hundred-Page Machine Learning Book
\emph default
.
 Andriy Burkov, 2019.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Hands-On ML"
literal "true"

\end_inset

A.
 Géron: 
\emph on
Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow 2nd Edition
\emph default
.
 O'Reilly Media, 2019
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Foundations of ML"
literal "true"

\end_inset

M.
 Mohri, A.
 Rostamizadeh, A.
 Talwalkar: 
\emph on
Foundations of Machine Learning
\emph default
.
 MIT Press, 2018.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "CKD: Global Dimensions and Perspectives"
literal "false"

\end_inset

Chronic kidney disease: global dimension and perspectives
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Analysis of Survival Data"
literal "false"

\end_inset

D.
 R.
 Cox, D.
 Oakes: 
\emph on
Analysis of Survival Data.
 
\emph default
Chapman & Hall, 1984.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Random Survival Forests"
literal "false"

\end_inset

H.
 Ishwaran, U.
 B.
 Kogalur, E.
 H.
 Blackstone.
 M.
 S.
 Lauer: 
\emph on
Random survival forests.
 
\emph default
Ann.
 Appl.
 Stat.
 2 (3) 841 - 860, September 2008
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "ML Crash Course"
literal "false"

\end_inset

Google Developers, 
\emph on
Machine Learning Crash Course, 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://developers.google.com/machine-learning/crash-course
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Survival Analysis self-learn"
literal "false"

\end_inset

D.
 G.
 Kleinbaum, M.
 Klein, 
\emph on
Survival Analysis: A Self-Learning Text 3rd Edition.
 
\emph default
Springer, 2012
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "scikit-survival"
literal "false"

\end_inset

S.
 Pölsterl, 
\emph on
scikit-survival:
\emph default
 
\emph on
A Library for Time-to-Event Analysis Built on Top of scikit-learn
\emph default
.
 Journal of Machine Learning Research, vol.
 21, no.
 212, pp.
 1–6, 2020.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "scikit-learn"
literal "false"

\end_inset

Pedregosa et al., 
\emph on
Scikit-learn: Machine Learning in Python
\emph default
, JMLR 12, pp.
 2825-2830, 2011
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Octoverse"
literal "false"

\end_inset

Github, 
\emph on
The state of open source software.
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://octoverse.github.com/2022/top-programming-languages
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Python docs"
literal "false"

\end_inset

Python Software Foundation, 
\emph on
Python Documentation, 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://docs.python.org/3/
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Python wiki"
literal "false"

\end_inset

The Python Wiki, 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://wiki.python.org/moin/
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "HELIOS"
literal "false"

\end_inset

Pavel Strachota, 
\emph on
HELIOS cluster documentation, 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://helios.fjfi.cvut.cz/
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Jupyter Notebook"
literal "false"

\end_inset

Project Jupyter, 
\emph on
Jupyter Notebook Documentation, 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://jupyter-notebook.readthedocs.io/en/latest/notebook.html
\end_layout

\end_inset


\end_layout

\end_body
\end_document
